---
title: "230331_dissertation_SVM"
author: "Xi Yang"
date: "2023-03-31"
output: html_document
---

# set up packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(tidyverse)
library(here)
library(rio)
library(tidybayes)
library(dplyr)
library(ggridges)
library(glue)
library(stringr)
library(forcats)
library(magrittr)
library(skimr)
library(janitor)
library(psych)
library(jpeg)
theme_set(theme_minimal())
library(knitr)
library(ggpubr)
library(rstatix)
library(kableExtra)
library(ggplot2)
```

# main dataset
```{r main data entry}
dir.create(here("data"))
dir.create(here("output"))
# folder <- "data"
# if (file.exists(folder)) {
# cat("The folder already exists")
# } else {
# dir.create(folder)
# }
fc_s1 <- import(here("data", "fc_session1.csv"), setclass = "tbl_df")
fc_s2 <- import(here("data", "fc_session2.csv"), setclass = "tbl_df")
fc_labels <- import(here("data", "FC_labels_plain.csv"), setclass = "tbl_df") %>%
  clean_names()

# 230520 cleaned datasets to be used
participant <- import(here("data", "participants.xlsx"), setclass = "tbl_df") %>%
  clean_names()
# 163*163 pairwise correlation values 85 participants; 2258365 cases at this point (163*163*85)

# 29 feature dataset before scaling
load(file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
# dataset name: test
```
```{r reshaping dataset1}
# due to >2 number of duplicated correlation values, can't use remove duplicate to keep half of the correlation matrix
fc_labels <- fc_labels[,2]
fc_labels[24,] <- "'networks.FrontoParietal.LPFC (R) (41,38,30)'"
fc_labels[48,] <- "'atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)'"
fc_labels[115,] <- "'atlas.HG r (Heschl Gyrus Right)'"
fc_labels[116,] <- "'atlas.HG r (Heschl Gyrus Left)'"

# https://stackoverflow.com/questions/8753531/repeat-rows-of-a-data-frame-n-times
new_labels_row <- bind_rows(replicate(85, fc_labels, simplify = FALSE))
# https://www.geeksforgeeks.org/convert-dataframe-column-to-list-in-r/
new_labels_list = as.list(new_labels_row)
names(fc_s1) <- new_labels_list$v2

fc_s1 <- bind_cols(fc_labels, fc_s1)

fc_s1 %<>% 
  rename(ROI = v2)

# change into long format to include sub IDs
fc_s1 <- fc_s1 %>% 
  pivot_longer(!ROI, names_to = c("drop", "ROI2", "ID_temp1"), names_sep = "'", values_to = "fc") %>% 
  select(-drop) %>% 
  separate(ID_temp1, c("drop", "ID_temp2")) %>% 
  select(-drop) %>% 
  mutate(ID = 
           case_when((as.numeric(ID_temp2) - 1)%%163 != 0 ~ (as.numeric(ID_temp2) - 1)%/%163 + 1,
                     (as.numeric(ID_temp2) - 1)%%163 == 0 ~ (as.numeric(ID_temp2) - 1)%/%163)) %>% 
  na.omit() %>% 
  select(-ID_temp2)
fc_s1 %>% 
  group_by(ID) %>% 
  summarise()
# 83 participants, without CONN20 & 23's data due to missing one session, note here the ID refers to CONN_ID, not original dataset ID

# names(participant)
#participant_label <- participant %>% 
#  mutate(ID = row_number()) 
participant_label <- participant %>% 
  select(c(sl_cond, conn_id)) %>% 
  rename(ID = conn_id)
fc_s1 <- left_join(fc_s1, participant_label, by = "ID")

# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
fc_s1 <- fc_s1 %>% 
  rename(sleep_deprivation = sl_cond)
fc_s1$sleep_deprivation <- as.factor(fc_s1$sleep_deprivation)
# str(fc_s1$sleep_deprivation)
fc_s1 <- fc_s1 %>% 
  unite("index", ROI:ROI2, remove = TRUE)

save(fc_s1, file = here("output", "230501fc_s1.Rdata"))
```
```{r reshaping dataset2}
# due to >2 number of duplicated correlation values, can't use remove duplicate to keep half of the correlation matrix
# fc_labels <- fc_labels[,2]
# # View(fc_labels)
# fc_labels[24,] <- "'networks.FrontoParietal.LPFC (R) (41,38,30)'"
# fc_labels[48,] <- "'atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)'"
# fc_labels[115,] <- "'atlas.HG r (Heschl Gyrus Right)'"
# fc_labels[116,] <- "'atlas.HG r (Heschl Gyrus Left)'"
#
## https://stackoverflow.com/questions/8753531/repeat-rows-of-a-data-frame-n-times
# new_labels_row <- bind_rows(replicate(85, fc_labels, simplify = FALSE))
## https://www.geeksforgeeks.org/convert-dataframe-column-to-list-in-r/
# new_labels_list = as.list(new_labels_row)
names(fc_s2) <- new_labels_list$v2

fc_s2 <- bind_cols(fc_labels, fc_s2)

fc_s2 %<>% 
  rename(ROI = v2)

# names(fc_s2)
# change into long format to include sub IDs
fc_s2 <- fc_s2 %>% 
  pivot_longer(!ROI, names_to = c("drop", "ROI2", "ID_temp1"), names_sep = "'", values_to = "fc") %>% 
  select(-drop) %>% 
  separate(ID_temp1, c("drop", "ID_temp2")) %>% 
  select(-drop) %>% 
  mutate(ID = 
           case_when((as.numeric(ID_temp2) - 1)%%163 != 0 ~ (as.numeric(ID_temp2) - 1)%/%163 + 1,
                     (as.numeric(ID_temp2) - 1)%%163 == 0 ~ (as.numeric(ID_temp2) - 1)%/%163)) %>% 
  na.omit() %>% 
  select(-ID_temp2)
fc_s2 %>% 
  group_by(ID) %>% 
  summarise()
# 79 participants, without CONN13, 18, 20, 35, 54, 82's data (all NAN based on matlab's s2 dataset)
# participant_label <- participant %>% 
#   select(c(sl_cond, conn_id)) %>% 
#   rename(ID = conn_id)
fc_s2 <- left_join(fc_s2, participant_label, by = "ID")

fc_s2 <- fc_s2 %>% 
  mutate(sleep_deprivation = case_when(sl_cond == 1 ~ 2,
                                       sl_cond == 2 ~ 1))
fc_s2$sleep_deprivation <- as.factor(fc_s2$sleep_deprivation)

fc_s2 <- fc_s2 %>% 
  unite("index", ROI:ROI2, remove = TRUE) %>% 
  select(-sl_cond)
save(fc_s2, file = here("output", "230501fc_s2.Rdata"))
```

```{r a priori selection of ROIs & networks (obselete)}
# a priori selection of ROIs & networks
# 163 network/atlas ROIs
# View(fc_s1)
# within DMN, dorsal attention, & ventral attention or salience networks
# within the r-inferior parietal lobule (angular gyrus & supramarginal gyrus)
# within the l-precuneus/posterior cingulate cortex 
# between the dorsal and ventral attention networks
# between the DMN & attention and salience/ventral attention networks
# r-intraparietal sulcus & superior parietal lobule with superior parietal lobule, r-intraparietal sulcus, inferior frontal gyrus, precentral gyrus, insula, causal & dorsal lateral occipital cortex, l-fusiform gyrus, thalamus, cerebellum
# amygdala with other regions
selected_index <- c("'networks.DefaultMode.LP (R) (47,-67,29)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DefaultMode.PCC (1,-61,38)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DefaultMode.PCC (1,-61,38)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.DorsalAttention.FEF (L)  (-27,-9,64)", 
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DorsalAttention.FEF (L)  (-27,-9,64)", 
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DorsalAttention.FEF (R)  (30,-6,64)", 
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DorsalAttention.FEF (L)  (-27,-9,64)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DorsalAttention.FEF (R)  (30,-6,64)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DorsalAttention.IPS (L)  (-39,-43,52)",
                    "'networks.Salience.AInsula (L) (-44,13,1)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.aSMG r (Supramarginal Gyrus, anterior division Right)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.pSMG r (Supramarginal Gyrus, posterior division Right)",
                    "'atlas.AG r (Angular Gyrus Right)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.Precuneous (Precuneous Cortex)'_atlas.PC (Cingulate Gyrus, posterior division)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.ACC (0,22,35)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.ACC (0,22,35)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.ACC (0,22,35)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.ACC (0,22,35)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.ACC (0,22,35)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.ACC (0,22,35)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.ACC (0,22,35)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.AInsula (L) (-44,13,1)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.AInsula (L) (-44,13,1)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.AInsula (L) (-44,13,1)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'atlas.Amygdala r'_networks.DefaultMode.PCC (1,-61,38)",
                    "'atlas.Amygdala r'_networks.Salience.ACC (0,22,35)",
                    "'atlas.Amygdala r'_networks.Language.IFG (R) (54,28,1)",
                    "'atlas.Amygdala r'_atlas.MidFG r (Middle Frontal Gyrus Right)",
                    "'atlas.Amygdala r'_atlas.MidFG l (Middle Frontal Gyrus Left)",
                    "'atlas.Amygdala r'_atlas.IFG tri r (Inferior Frontal Gyrus, pars triangularis Right)",
                    "'atlas.Amygdala r'_atlas.IFG oper r (Inferior Frontal Gyrus, pars opercularis Right)",
                    "'atlas.Amygdala r'_atlas.PreCG r (Precentral Gyrus Right)",
                    "'atlas.Amygdala r'_atlas.MedFC (Frontal Medial Cortex)",
                    "'atlas.Amygdala r'_atlas.AC (Cingulate Gyrus, anterior division)",
                    "'atlas.Amygdala r'_atlas.PC (Cingulate Gyrus, posterior division)",
                    "'atlas.Amygdala r'_atlas.Precuneous (Precuneous Cortex)",
                    "'atlas.Amygdala r'_atlas.aPaHC r (Parahippocampal Gyrus, anterior division Right)",
                    "'atlas.Amygdala r'_atlas.pPaHC r (Parahippocampal Gyrus, posterior division Right)",
                    "'atlas.Amygdala r'_atlas.SFG r (Superior Frontal Gyrus Right)",
                    "'atlas.Amygdala l'_networks.DefaultMode.PCC (1,-61,38)",
                    "'atlas.Amygdala l'_networks.Salience.ACC (0,22,35)",
                    "'atlas.Amygdala l'_networks.Language.IFG (R) (54,28,1)",
                    "'atlas.Amygdala l'_atlas.MidFG r (Middle Frontal Gyrus Right)",
                    "'atlas.Amygdala l'_atlas.MidFG l (Middle Frontal Gyrus Left)",
                    "'atlas.Amygdala l'_atlas.IFG tri r (Inferior Frontal Gyrus, pars triangularis Right)",
                    "'atlas.Amygdala l'_atlas.IFG oper r (Inferior Frontal Gyrus, pars opercularis Right)",
                    "'atlas.Amygdala l'_atlas.PreCG r (Precentral Gyrus Right)",
                    "'atlas.Amygdala l'_atlas.AC (Cingulate Gyrus, anterior division)",
                    "'atlas.Amygdala l'_atlas.PC (Cingulate Gyrus, posterior division)",
                    "'atlas.Amygdala l'_atlas.Precuneous (Precuneous Cortex)",
                    "'atlas.Amygdala l'_atlas.aPaHC r (Parahippocampal Gyrus, anterior division Right)",
                    "'atlas.Amygdala l'_atlas.pPaHC r (Parahippocampal Gyrus, posterior division Right)",
                    "'atlas.Amygdala l'_atlas.SFG r (Superior Frontal Gyrus Right)",
                    "'atlas.AG r (Angular Gyrus Right)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.aSMG l (Supramarginal Gyrus, anterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.pSMG l (Supramarginal Gyrus, posterior division Left)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.aSMG r (Supramarginal Gyrus, anterior division Right)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.pSMG r (Supramarginal Gyrus, posterior division Right)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.AG r (Angular Gyrus Right)",
                    "'atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.aSTG l (Superior Temporal Gyrus, anterior division Left)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.pSTG r (Superior Temporal Gyrus, posterior division Right)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.pSTG l (Superior Temporal Gyrus, posterior division Left)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.aSMG l (Supramarginal Gyrus, anterior division Left)'_atlas.aSTG l (Superior Temporal Gyrus, anterior division Left)",
                    "'atlas.aSMG l (Supramarginal Gyrus, anterior division Left)'_atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)",
                    "'atlas.aSMG l (Supramarginal Gyrus, anterior division Left)'_atlas.pSTG r (Superior Temporal Gyrus, posterior division Right)",
                    "'atlas.aSMG l (Supramarginal Gyrus, anterior division Left)'_atlas.pSTG l (Superior Temporal Gyrus, posterior division Left)",
                    "'atlas.pSMG l (Supramarginal Gyrus, posterior division Left)'_atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)",
                    "'atlas.pSMG l (Supramarginal Gyrus, posterior division Left)'_atlas.aSTG l (Superior Temporal Gyrus, anterior division Left)",
                    "'atlas.pSMG l (Supramarginal Gyrus, posterior division Left)'_atlas.pSTG r (Superior Temporal Gyrus, posterior division Right)",
                    "'atlas.pSMG l (Supramarginal Gyrus, posterior division Left)'_atlas.pSTG l (Superior Temporal Gyrus, posterior division Left)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.aSTG l (Superior Temporal Gyrus, anterior division Left)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.pSTG r (Superior Temporal Gyrus, posterior division Right)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.pSTG l (Superior Temporal Gyrus, posterior division Left)",
                    "'atlas.PC (Cingulate Gyrus, posterior division)'_atlas.aSMG r (Supramarginal Gyrus, anterior division Right)",
                    "'atlas.Precuneous (Precuneous Cortex)'_atlas.aSMG r (Supramarginal Gyrus, anterior division Right)",
                    "'atlas.PC (Cingulate Gyrus, posterior division)'_atlas.pSMG r (Supramarginal Gyrus, posterior division Right)",
                    "'atlas.Precuneous (Precuneous Cortex)'_atlas.pSMG r (Supramarginal Gyrus, posterior division Right)",
                    "'atlas.PC (Cingulate Gyrus, posterior division)'_atlas.AG r (Angular Gyrus Right)",
                    "'atlas.Precuneous (Precuneous Cortex)'_atlas.AG r (Angular Gyrus Right)",
                    "'atlas.PC (Cingulate Gyrus, posterior division)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.Precuneous (Precuneous Cortex)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_atlas.aSMG l (Supramarginal Gyrus, anterior division Left)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_atlas.pSMG l (Supramarginal Gyrus, posterior division Left)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_atlas.AG l (Angular Gyrus Left)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.aMTG l (Middle Temporal Gyrus, anterior division Left)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.pMTG l (Middle Temporal Gyrus, posterior division Left)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.toMTG l (Middle Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.aMTG l (Middle Temporal Gyrus, anterior division Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.pMTG l (Middle Temporal Gyrus, posterior division Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.toMTG l (Middle Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.aMTG l (Middle Temporal Gyrus, anterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.pMTG l (Middle Temporal Gyrus, posterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.toMTG l (Middle Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.aMTG l (Middle Temporal Gyrus, anterior division Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.pMTG l (Middle Temporal Gyrus, posterior division Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.toMTG l (Middle Temporal Gyrus, temporooccipital part Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.aITG l (Inferior Temporal Gyrus, anterior division Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.pITG l (Inferior Temporal Gyrus, posterior division Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.toITG l (Inferior Temporal Gyrus, temporooccipital part Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.aITG l (Inferior Temporal Gyrus, anterior division Left)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.pITG l (Inferior Temporal Gyrus, posterior division Left)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.toITG l (Inferior Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.aITG l (Inferior Temporal Gyrus, anterior division Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.pITG l (Inferior Temporal Gyrus, posterior division Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.toITG l (Inferior Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.aITG l (Inferior Temporal Gyrus, anterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.pITG l (Inferior Temporal Gyrus, posterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.toITG l (Inferior Temporal Gyrus, temporooccipital part Left)",
                    "'networks.Language.IFG (L) (-51,26,2)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'networks.Language.IFG (R) (54,28,1)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'networks.Cerebellar.Anterior (0,-63,-30)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'networks.Cerebellar.Posterior (0,-79,-32)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IC r (Insular Cortex Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IC l (Insular Cortex Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IFG tri r (Inferior Frontal Gyrus, pars triangularis Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IFG tri l (Inferior Frontal Gyrus, pars triangularis Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IFG oper r (Inferior Frontal Gyrus, pars opercularis Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IFG oper l (Inferior Frontal Gyrus, pars opercularis Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.PreCG r (Precentral Gyrus Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.PreCG l (Precentral Gyrus Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.SPL l (Superior Parietal Lobule Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Thalamus r'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Thalamus l'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb1 l (Cerebelum Crus1 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb1 r (Cerebelum Crus1 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb2 l (Cerebelum Crus2 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb2 r (Cerebelum Crus2 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb3 l (Cerebelum 3 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb3 r (Cerebelum 3 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb45 l (Cerebelum 4 5 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb45 r (Cerebelum 4 5 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb6 l (Cerebelum 6 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb6 r (Cerebelum 6 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb7 l (Cerebelum 7b Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb7 r (Cerebelum 7b Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb8 l (Cerebelum 8 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb8 r (Cerebelum 8 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb9 l (Cerebelum 9 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb9 r (Cerebelum 9 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb10 l (Cerebelum 10 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb10 r (Cerebelum 10 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.sLOC r (Lateral Occipital Cortex, superior division Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.sLOC l (Lateral Occipital Cortex, superior division Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.iLOC r (Lateral Occipital Cortex, inferior division Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.iLOC l (Lateral Occipital Cortex, inferior division Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.Language.IFG (L) (-51,26,2)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.Language.IFG (R) (54,28,1)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.Cerebellar.Anterior (0,-63,-30)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.Cerebellar.Posterior (0,-79,-32)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IC r (Insular Cortex Right)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IC l (Insular Cortex Left)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IFG tri r (Inferior Frontal Gyrus, pars triangularis Right)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IFG tri l (Inferior Frontal Gyrus, pars triangularis Left)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IFG oper r (Inferior Frontal Gyrus, pars opercularis Right)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IFG oper l (Inferior Frontal Gyrus, pars opercularis Left)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.PreCG r (Precentral Gyrus Right)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.PreCG l (Precentral Gyrus Left)",
                    "'atlas.sLOC r (Lateral Occipital Cortex, superior division Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.sLOC l (Lateral Occipital Cortex, superior division Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.iLOC r (Lateral Occipital Cortex, inferior division Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.iLOC l (Lateral Occipital Cortex, inferior division Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Thalamus r'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Thalamus l'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb1 l (Cerebelum Crus1 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb1 r (Cerebelum Crus1 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb2 l (Cerebelum Crus2 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb2 r (Cerebelum Crus2 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb3 l (Cerebelum 3 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb3 r (Cerebelum 3 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb45 l (Cerebelum 4 5 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb45 r (Cerebelum 4 5 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb6 l (Cerebelum 6 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb6 r (Cerebelum 6 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb7 l (Cerebelum 7b Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb7 r (Cerebelum 7b Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb8 l (Cerebelum 8 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb8 r (Cerebelum 8 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb9 l (Cerebelum 9 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb9 r (Cerebelum 9 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb10 l (Cerebelum 10 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb10 r (Cerebelum 10 Right)'_atlas.SPL r (Superior Parietal Lobule Right)")

# sum(duplicated(selected_index))
# no duplicates

# inspect duplicated fc values, just in case same ROI under different names were selected
# conclusion: even though some fc values are the same, they exist not due to same ROIs under different names were selected
# library(NCmisc)
# test <- fc_s1
# test %>% 
#   group_by(fc) %>% 
#   count() %>% 
#   filter(n!=1) %>% 
#   describe()
# # 181496 duplicated fc values
# test_sub1 <- fc_s1 %>% 
#   filter(ID == 1)
# test_sub1 %>% 
#   group_by(fc) %>% 
#   count() %>% 
#   filter(n!=1) %>% 
#   describe()
# dup.pairs(test_sub1$fc)
# test_sub1_dup <- test_sub1[NCmisc::dup.pairs(test_sub1$fc),]

# later to do: fronto parietal (I add this due to high relevance in task-based sleep deprivation studies; may delete later if feature numbers grows too large), within and between may need to be examined
#"'networks.FrontoParietal.PPC (L)  (-46,-58,49)'_networks.FrontoParietal.LPFC (L)  (-43,33,28)",
#                    "'networks.FrontoParietal.LPFC (R) (41,38,30)'_networks.FrontoParietal.LPFC (L)  (-43,33,28)",
#                    "'networks.FrontoParietal.LPFC (R) (41,38,30)'_networks.FrontoParietal.PPC (L)  (-46,-58,49)",
#                    "'networks.FrontoParietal.PPC (R)  (52,-52,45)'_networks.FrontoParietal.LPFC (L)  (-43,33,28)",
#                    "'networks.FrontoParietal.PPC (R)  (52,-52,45)'_networks.FrontoParietal.PPC (L)  (-46,-58,49)",
#                    "'networks.FrontoParietal.PPC (R)  (52,-52,45)'_networks.FrontoParietal.LPFC (R) (41,38,30)",
```

```{r 230507 updated a priori selection of ROIs & networks}
# a priori selection of ROIs & networks
# 163 network/atlas ROIs
# View(fc_s1)
# within DMN, dorsal attention, & ventral attention or salience networks
# within the r-inferior parietal lobule (angular gyrus & supramarginal gyrus)
# within the l-precuneus/posterior cingulate cortex 
# between the dorsal and ventral attention networks
# between the DMN & attention and salience/ventral attention networks
# r-intraparietal sulcus & superior parietal lobule with superior parietal lobule, r-intraparietal sulcus, inferior frontal gyrus, precentral gyrus, insula, causal & dorsal lateral occipital cortex, l-fusiform gyrus, thalamus, cerebellum
# amygdala with other regions
#load(file = here("output", "230501fc_s1.Rdata"))
#temp <- as.data.frame(unique(fc_s1$index))
#View(as.dataframe(temp))
selected_index_updated <- c("'networks.DefaultMode.LP (R) (47,-67,29)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DefaultMode.PCC (1,-61,38)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DefaultMode.PCC (1,-61,38)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.DorsalAttention.FEF (L)  (-27,-9,64)", 
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DorsalAttention.FEF (L)  (-27,-9,64)", 
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DorsalAttention.FEF (R)  (30,-6,64)", 
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DorsalAttention.FEF (L)  (-27,-9,64)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DorsalAttention.FEF (R)  (30,-6,64)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DorsalAttention.IPS (L)  (-39,-43,52)",
                    "'networks.Salience.AInsula (L) (-44,13,1)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.ACC (0,22,35)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.aSMG r (Supramarginal Gyrus, anterior division Right)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.pSMG r (Supramarginal Gyrus, posterior division Right)",
                    "'atlas.AG r (Angular Gyrus Right)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.Precuneous (Precuneous Cortex)'_atlas.PC (Cingulate Gyrus, posterior division)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.ACC (0,22,35)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.ACC (0,22,35)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.ACC (0,22,35)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.ACC (0,22,35)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.AInsula (L) (-44,13,1)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.AInsula (R) (47,14,0)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.RPFC (L) (-32,45,27)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.RPFC (R) (32,46,27)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.FEF (L)  (-27,-9,64)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.FEF (R)  (30,-6,64)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.IPS (L)  (-39,-43,52)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.DorsalAttention.IPS (R)  (39,-42,54)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.ACC (0,22,35)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.ACC (0,22,35)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.ACC (0,22,35)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.AInsula (L) (-44,13,1)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.AInsula (L) (-44,13,1)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.AInsula (L) (-44,13,1)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.AInsula (R) (47,14,0)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.RPFC (L) (-32,45,27)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.RPFC (R) (32,46,27)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.SMG (L) (-60,-39,31)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.DefaultMode.LP (L) (-39,-77,33)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.DefaultMode.LP (R) (47,-67,29)",
                    "'networks.Salience.SMG (R) (62,-35,32)'_networks.DefaultMode.PCC (1,-61,38)",
                    "'atlas.Amygdala r'_networks.DefaultMode.PCC (1,-61,38)",
                    "'atlas.Amygdala r'_networks.Salience.ACC (0,22,35)",
                    "'atlas.Amygdala r'_networks.Language.IFG (R) (54,28,1)",
                    "'atlas.Amygdala r'_atlas.MidFG r (Middle Frontal Gyrus Right)",
                    "'atlas.Amygdala r'_atlas.MidFG l (Middle Frontal Gyrus Left)",
                    "'atlas.Amygdala r'_atlas.IFG tri r (Inferior Frontal Gyrus, pars triangularis Right)",
                    "'atlas.Amygdala r'_atlas.IFG oper r (Inferior Frontal Gyrus, pars opercularis Right)",
                    "'atlas.Amygdala r'_atlas.PreCG r (Precentral Gyrus Right)",
                    "'atlas.Amygdala r'_atlas.MedFC (Frontal Medial Cortex)",
                    "'atlas.Amygdala r'_atlas.AC (Cingulate Gyrus, anterior division)",
                    "'atlas.Amygdala r'_atlas.PC (Cingulate Gyrus, posterior division)",
                    "'atlas.Amygdala r'_atlas.Precuneous (Precuneous Cortex)",
                    "'atlas.Amygdala r'_atlas.aPaHC r (Parahippocampal Gyrus, anterior division Right)",
                    "'atlas.Amygdala r'_atlas.pPaHC r (Parahippocampal Gyrus, posterior division Right)",
                    "'atlas.Amygdala r'_atlas.SFG r (Superior Frontal Gyrus Right)",
                    "'atlas.Amygdala l'_networks.DefaultMode.PCC (1,-61,38)",
                    "'atlas.Amygdala l'_networks.Salience.ACC (0,22,35)",
                    "'atlas.Amygdala l'_networks.Language.IFG (R) (54,28,1)",
                    "'atlas.Amygdala l'_atlas.MidFG r (Middle Frontal Gyrus Right)",
                    "'atlas.Amygdala l'_atlas.MidFG l (Middle Frontal Gyrus Left)",
                    "'atlas.Amygdala l'_atlas.IFG tri r (Inferior Frontal Gyrus, pars triangularis Right)",
                    "'atlas.Amygdala l'_atlas.IFG oper r (Inferior Frontal Gyrus, pars opercularis Right)",
                    "'atlas.Amygdala l'_atlas.PreCG r (Precentral Gyrus Right)",
                    "'atlas.Amygdala l'_atlas.AC (Cingulate Gyrus, anterior division)",
                    "'atlas.Amygdala l'_atlas.PC (Cingulate Gyrus, posterior division)",
                    "'atlas.Amygdala l'_atlas.Precuneous (Precuneous Cortex)",
                    "'atlas.Amygdala l'_atlas.aPaHC r (Parahippocampal Gyrus, anterior division Right)",
                    "'atlas.Amygdala l'_atlas.pPaHC r (Parahippocampal Gyrus, posterior division Right)",
                    "'atlas.Amygdala r'_atlas.SFG l (Superior Frontal Gyrus Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.aSMG l (Supramarginal Gyrus, anterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.pSMG l (Supramarginal Gyrus, posterior division Left)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.aSMG r (Supramarginal Gyrus, anterior division Right)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.pSMG r (Supramarginal Gyrus, posterior division Right)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.AG r (Angular Gyrus Right)",
                    "'atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.aSTG l (Superior Temporal Gyrus, anterior division Left)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.pSTG r (Superior Temporal Gyrus, posterior division Right)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.pSTG l (Superior Temporal Gyrus, posterior division Left)'_networks.Salience.SMG (L) (-60,-39,31)",
                    "'atlas.aSMG l (Supramarginal Gyrus, anterior division Left)'_atlas.aSTG l (Superior Temporal Gyrus, anterior division Left)",
                    "'atlas.aSMG l (Supramarginal Gyrus, anterior division Left)'_atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)",
                    "'atlas.aSMG l (Supramarginal Gyrus, anterior division Left)'_atlas.pSTG r (Superior Temporal Gyrus, posterior division Right)",
                    "'atlas.aSMG l (Supramarginal Gyrus, anterior division Left)'_atlas.pSTG l (Superior Temporal Gyrus, posterior division Left)",
                    "'atlas.pSMG l (Supramarginal Gyrus, posterior division Left)'_atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)",
                    "'atlas.pSMG l (Supramarginal Gyrus, posterior division Left)'_atlas.aSTG l (Superior Temporal Gyrus, anterior division Left)",
                    "'atlas.pSMG l (Supramarginal Gyrus, posterior division Left)'_atlas.pSTG r (Superior Temporal Gyrus, posterior division Right)",
                    "'atlas.pSMG l (Supramarginal Gyrus, posterior division Left)'_atlas.pSTG l (Superior Temporal Gyrus, posterior division Left)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.aSTG r (Superior Temporal Gyrus, anterior division Right)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.aSTG l (Superior Temporal Gyrus, anterior division Left)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.pSTG r (Superior Temporal Gyrus, posterior division Right)",
                    "'atlas.AG l (Angular Gyrus Left)'_atlas.pSTG l (Superior Temporal Gyrus, posterior division Left)",
                    "'atlas.PC (Cingulate Gyrus, posterior division)'_atlas.aSMG r (Supramarginal Gyrus, anterior division Right)",
                    "'atlas.Precuneous (Precuneous Cortex)'_atlas.aSMG r (Supramarginal Gyrus, anterior division Right)",
                    "'atlas.PC (Cingulate Gyrus, posterior division)'_atlas.pSMG r (Supramarginal Gyrus, posterior division Right)",
                    "'atlas.Precuneous (Precuneous Cortex)'_atlas.pSMG r (Supramarginal Gyrus, posterior division Right)",
                    "'atlas.PC (Cingulate Gyrus, posterior division)'_atlas.AG r (Angular Gyrus Right)",
                    "'atlas.Precuneous (Precuneous Cortex)'_atlas.AG r (Angular Gyrus Right)",
                    "'atlas.PC (Cingulate Gyrus, posterior division)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.Precuneous (Precuneous Cortex)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_atlas.aSMG r (Supramarginal Gyrus, anterior division Right)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_atlas.pSMG r (Supramarginal Gyrus, posterior division Right)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_atlas.AG r (Angular Gyrus Right)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.aMTG l (Middle Temporal Gyrus, anterior division Left)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.pMTG l (Middle Temporal Gyrus, posterior division Left)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.toMTG l (Middle Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.aMTG l (Middle Temporal Gyrus, anterior division Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.pMTG l (Middle Temporal Gyrus, posterior division Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.toMTG l (Middle Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.aMTG l (Middle Temporal Gyrus, anterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.pMTG l (Middle Temporal Gyrus, posterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.toMTG l (Middle Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.aMTG l (Middle Temporal Gyrus, anterior division Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.pMTG l (Middle Temporal Gyrus, posterior division Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.toMTG l (Middle Temporal Gyrus, temporooccipital part Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.aITG l (Inferior Temporal Gyrus, anterior division Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.pITG l (Inferior Temporal Gyrus, posterior division Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.toITG l (Inferior Temporal Gyrus, temporooccipital part Left)'_networks.Salience.SMG (R) (62,-35,32)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.aITG l (Inferior Temporal Gyrus, anterior division Left)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.pITG l (Inferior Temporal Gyrus, posterior division Left)",
                    "'atlas.aSMG r (Supramarginal Gyrus, anterior division Right)'_atlas.toITG l (Inferior Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.aITG l (Inferior Temporal Gyrus, anterior division Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.pITG l (Inferior Temporal Gyrus, posterior division Left)",
                    "'atlas.pSMG r (Supramarginal Gyrus, posterior division Right)'_atlas.toITG l (Inferior Temporal Gyrus, temporooccipital part Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.aITG l (Inferior Temporal Gyrus, anterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.pITG l (Inferior Temporal Gyrus, posterior division Left)",
                    "'atlas.AG r (Angular Gyrus Right)'_atlas.toITG l (Inferior Temporal Gyrus, temporooccipital part Left)",
                    "'networks.Language.IFG (L) (-51,26,2)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'networks.Language.IFG (R) (54,28,1)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'networks.Cerebellar.Anterior (0,-63,-30)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'networks.Cerebellar.Posterior (0,-79,-32)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IC r (Insular Cortex Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IC l (Insular Cortex Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IFG tri r (Inferior Frontal Gyrus, pars triangularis Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IFG tri l (Inferior Frontal Gyrus, pars triangularis Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IFG oper r (Inferior Frontal Gyrus, pars opercularis Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.IFG oper l (Inferior Frontal Gyrus, pars opercularis Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.PreCG r (Precentral Gyrus Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.PreCG l (Precentral Gyrus Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.SPL l (Superior Parietal Lobule Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Thalamus r'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Thalamus l'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb1 l (Cerebelum Crus1 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb1 r (Cerebelum Crus1 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb2 l (Cerebelum Crus2 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb2 r (Cerebelum Crus2 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb3 l (Cerebelum 3 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb3 r (Cerebelum 3 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb45 l (Cerebelum 4 5 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb45 r (Cerebelum 4 5 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb6 l (Cerebelum 6 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb6 r (Cerebelum 6 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb7 l (Cerebelum 7b Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb7 r (Cerebelum 7b Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb8 l (Cerebelum 8 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb8 r (Cerebelum 8 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb9 l (Cerebelum 9 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb9 r (Cerebelum 9 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb10 l (Cerebelum 10 Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.Cereb10 r (Cerebelum 10 Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.sLOC r (Lateral Occipital Cortex, superior division Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.sLOC l (Lateral Occipital Cortex, superior division Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.iLOC r (Lateral Occipital Cortex, inferior division Right)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.iLOC l (Lateral Occipital Cortex, inferior division Left)'_networks.DorsalAttention.IPS (R)  (39,-42,54)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.Language.IFG (L) (-51,26,2)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.Language.IFG (R) (54,28,1)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.Cerebellar.Anterior (0,-63,-30)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_networks.Cerebellar.Posterior (0,-79,-32)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IC r (Insular Cortex Right)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IC l (Insular Cortex Left)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IFG tri r (Inferior Frontal Gyrus, pars triangularis Right)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IFG tri l (Inferior Frontal Gyrus, pars triangularis Left)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IFG oper r (Inferior Frontal Gyrus, pars opercularis Right)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.IFG oper l (Inferior Frontal Gyrus, pars opercularis Left)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.PreCG r (Precentral Gyrus Right)",
                    "'atlas.SPL r (Superior Parietal Lobule Right)'_atlas.PreCG l (Precentral Gyrus Left)",
                    "'atlas.sLOC r (Lateral Occipital Cortex, superior division Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.sLOC l (Lateral Occipital Cortex, superior division Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.iLOC r (Lateral Occipital Cortex, inferior division Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.iLOC l (Lateral Occipital Cortex, inferior division Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.OFusG l (Occipital Fusiform Gyrus Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Thalamus r'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Thalamus l'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb1 l (Cerebelum Crus1 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb1 r (Cerebelum Crus1 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb2 l (Cerebelum Crus2 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb2 r (Cerebelum Crus2 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb3 l (Cerebelum 3 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb3 r (Cerebelum 3 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb45 l (Cerebelum 4 5 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb45 r (Cerebelum 4 5 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb6 l (Cerebelum 6 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb6 r (Cerebelum 6 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb7 l (Cerebelum 7b Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb7 r (Cerebelum 7b Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb8 l (Cerebelum 8 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb8 r (Cerebelum 8 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb9 l (Cerebelum 9 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb9 r (Cerebelum 9 Right)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb10 l (Cerebelum 10 Left)'_atlas.SPL r (Superior Parietal Lobule Right)",
                    "'atlas.Cereb10 r (Cerebelum 10 Right)'_atlas.SPL r (Superior Parietal Lobule Right)")

# reduce the number of features by averaging adjacent regions
sink(here("output",'selected_index_updated.csv'))
print(selected_index_updated)
sink()
# sum(duplicated(selected_index_updated))
# no duplicates

# inspect duplicated fc values, just in case same ROI under different names were selected
# conclusion: even though some fc values are the same, they exist not due to same ROIs under different names were selected
# library(NCmisc)
# test <- fc_s1
# test %>% 
#   group_by(fc) %>% 
#   count() %>% 
#   filter(n!=1) %>% 
#   describe()
# # 181496 duplicated fc values
# test_sub1 <- fc_s1 %>% 
#   filter(ID == 1)
# test_sub1 %>% 
#   group_by(fc) %>% 
#   count() %>% 
#   filter(n!=1) %>% 
#   describe()
# dup.pairs(test_sub1$fc)
# test_sub1_dup <- test_sub1[NCmisc::dup.pairs(test_sub1$fc),]

# later to do: fronto parietal (I add this due to high relevance in task-based sleep deprivation studies; may delete later if feature numbers grows too large), within and between may need to be examined
#"'networks.FrontoParietal.PPC (L)  (-46,-58,49)'_networks.FrontoParietal.LPFC (L)  (-43,33,28)",
#                    "'networks.FrontoParietal.LPFC (R) (41,38,30)'_networks.FrontoParietal.LPFC (L)  (-43,33,28)",
#                    "'networks.FrontoParietal.LPFC (R) (41,38,30)'_networks.FrontoParietal.PPC (L)  (-46,-58,49)",
#                    "'networks.FrontoParietal.PPC (R)  (52,-52,45)'_networks.FrontoParietal.LPFC (L)  (-43,33,28)",
#                    "'networks.FrontoParietal.PPC (R)  (52,-52,45)'_networks.FrontoParietal.PPC (L)  (-46,-58,49)",
#                    "'networks.FrontoParietal.PPC (R)  (52,-52,45)'_networks.FrontoParietal.LPFC (R) (41,38,30)",
```
```{r check scripts 230511 generate full dataset feature 258}
load(file = here("output", "230501fc_s1.Rdata"))
# View(fc_s1)
load(file = here("output", "230501fc_s2.Rdata"))
# fc_s2
fc_s1_ml <- fc_s1 %>% 
  filter(index %in% selected_index_updated) %>% 
  pivot_wider(names_from = index, values_from = fc)
fc_s2_ml <- fc_s2 %>% 
  filter(index %in% selected_index_updated) %>% 
  pivot_wider(names_from = index, values_from = fc)
# visit/session 1, n=83; visit/session 2, n=79
fc_ml <- bind_rows(fc_s1_ml, fc_s2_ml)
test <- fc_ml %>% 
  filter(!ID %in% c(22, 53, 66))
View(test)
# those participants' scan data had significant outliers based on QA
save(test, file = here("output", "230511ML_dataset_258_feature.Rdata"))
# remove 13 (sub-9026), 18 (sub-9022), 23 (sub-9029), 35 (sub-9044), 54 (sub-9066), 82 (sub-9095) for now as they only have one session, this way the two conditions are completely balanced; also sub-9025 due to missing the 2nd scans in both sessions, always removed prior to this step
test <- test %>% 
  filter(!ID %in% c(13, 18, 23, 35, 54, 82)) 
# 150/2 = 75 pairs of participants
save(test, file = here("output", "230511ML_dataset_258_feature_full_balance.Rdata"))
```
```{r check script 230507 updated merge & split machine learning dataset average ROIs Feature 29}
load(file = here("output", "230511ML_dataset_258_feature_full_balance.Rdata"))
colnames(test)[3:5] = "w_DMN"
colnames(test)[6:11] = "w_DAN"
colnames(test)[12:32] = "w_SAN"
colnames(test)[33:35] = "w_r-IPL"
colnames(test)[36] = "w_l-precuneus-PCC"
colnames(test)[37:64] = "b_DAN_SAN"
colnames(test)[65:76] = "b_DAN_DMN"
colnames(test)[77:97] = "b_SAN_DMN"
colnames(test)[c(98, 108, 109, 113, 122, 123)] = "AMG_r-dPCC"
colnames(test)[c(99, 107, 114, 121)] = "AMG_r-dACC"
colnames(test)[c(100, 103, 104, 115, 118, 119)] = "AMG_r-IFG"
colnames(test)[c(101, 102, 116, 117)] = "AMG_dlPFC"
colnames(test)[c(105, 120)] = "AMG_r-precentral-gyrus"
colnames(test)[c(106, 112, 126)] = "r-AMG_r-mPFC"
colnames(test)[c(110, 111, 124, 125)] = "AMG_r-parahippocampal-gyrus"
colnames(test)[127:132] = "l-IPL_r-IPL"
colnames(test)[133:148] = "l-IPL_superior-temporal-gyrus"
colnames(test)[149:156] = "r-IPL_l-precuneus"
colnames(test)[157:160] = "r-IPL_l-fusiform-gyrus"
colnames(test)[161:172] = "r-IPL_l-middle-temporal-gyrus"
colnames(test)[173:184] = "r-IPL_l-inferior-temporal-gyrus"
colnames(test)[c(185, 186, 191, 192, 193, 194, 224, 225, 230:233)] = "r-IPS_IFG"
colnames(test)[c(187, 188, 202:219, 226, 227, 243:260)] = "r-IPS_cerebellum"
colnames(test)[c(189, 190, 228, 229)] = "r-IPS_insula"
colnames(test)[c(195, 196, 234, 235)] = "r-IPS_precentral-gyrus"
colnames(test)[c(197, 198)] = "r-IPS_SPL"
colnames(test)[c(199, 240)] = "r-IPS_l-fusiform-gyrus"
colnames(test)[c(200, 201, 241, 242)] = "r-IPS_thalamus"
colnames(test)[c(220:223, 236:239)] = "r-IPS_caudal-dorsal-lateral-occipital-cortex"
# 29 features in total
# https://stackoverflow.com/questions/43893955/dplyr-mutate-solve-unique-names-error
test <- test %>% 
  setNames(make.names(names(.), unique = TRUE)) %>% 
  clean_names()
test <- mutate(test, 
                w_DMN_fc = rowMeans(select(test, contains("w_dmn")), 
                                    na.rm = TRUE),
                w_DAN_fc = rowMeans(select(test, contains("w_dan")), 
                                    na.rm = TRUE),
                w_SAN_fc = rowMeans(select(test, contains("w_san")), 
                                    na.rm = TRUE),
                w_r_IPL_fc = rowMeans(select(test, contains("w_r_ipl")), 
                                    na.rm = TRUE),
                w_l_precuneus_PCC_fc = rowMeans(select(test, contains("w_l_precuneus_pcc")), na.rm = TRUE),
                b_DAN_SAN_fc = rowMeans(select(test, contains("b_dan_san")), 
                                    na.rm = TRUE),
                b_DAN_DMN_fc = rowMeans(select(test, contains("b_dan_dmn")), 
                                    na.rm = TRUE),
                b_SAN_DMN_fc = rowMeans(select(test, contains("b_san_dmn")), 
                                    na.rm = TRUE),
                AMG_r_d_PCC_fc = rowMeans(select(test, contains("amg_r_d_pcc")), na.rm = TRUE),
                AMG_r_d_ACC_fc = rowMeans(select(test, contains("amg_r_d_acc")), na.rm = TRUE),
                AMG_r_IFG_fc = rowMeans(select(test, contains("amg_r_ifg")), na.rm = TRUE),
                AMG_dl_PFC_fc = rowMeans(select(test, contains("amg_dl_pfc")), 
                                    na.rm = TRUE),
                AMG_r_precentral_gyrus_fc = rowMeans(select(test, contains("amg_r_precentral_gyrus")), na.rm = TRUE),
                r_AMG_r_m_PFC_fc = rowMeans(select(test, contains("r_amg_r_m_pfc")), na.rm = TRUE),
                AMG_r_parahippocampal_gyrus_fc = rowMeans(select(test, contains("amg_r_parahippocampal_gyrus")), na.rm = TRUE),
                l_IPL_r_IPL_fc = rowMeans(select(test, contains("l_ipl_r_ipl")), na.rm = TRUE),
                l_IPL_superior_temporal_gyrus_fc = rowMeans(select(test, contains("l_ipl_superior_temporal_gyrus")), na.rm = TRUE),
                r_IPL_l_precuneus_fc = rowMeans(select(test, contains("r_ipl_l_precuneus")), na.rm = TRUE),
                r_IPL_l_fusiform_gyrus_fc = rowMeans(select(test, contains("r_ipl_l_fusiform_gyrus")), na.rm = TRUE),
                r_IPL_l_middle_temporal_gyrus_fc = rowMeans(select(test, contains("r_ipl_l_middle_temporal_gyrus")), na.rm = TRUE),
                r_IPL_l_inferior_temporal_gyrus_fc = rowMeans(select(test, contains("r_ipl_l_inferior_temporal_gyrus")), na.rm = TRUE),
                r_IPS_IFG_fc = rowMeans(select(test, contains("r_ips_ifg")), na.rm = TRUE),
                r_IPS_cerebellum_fc = rowMeans(select(test, contains("r_ips_cerebellum")), na.rm = TRUE),
                r_IPS_insula_fc = rowMeans(select(test, contains("r_ips_insula")), na.rm = TRUE),
                r_IPS_precentral_gyrus_fc = rowMeans(select(test, contains("r_ips_precentral_gyrus")), na.rm = TRUE),
                r_IPS_SPL_fc = rowMeans(select(test, contains("r_ips_spl")), na.rm = TRUE),
                r_IPS_l_fusiform_gyrus_fc = rowMeans(select(test, contains("r_ips_l_fusiform_gyrus")), na.rm = TRUE),
                r_IPS_thalamus_fc = rowMeans(select(test, contains("r_ips_thalamus")), na.rm = TRUE),
                r_IPS_caudal_dorsal_lateral_occipital_cortex_fc = rowMeans(select(test, contains("r_ips_caudal_dorsal_lateral_occipital_cortex")), na.rm = TRUE)) %>% 
  select(c(1, 2), ends_with("_fc"))

#str(fc_s1_ml)
#str(fc_s2_ml)
#str(test)
test <- test %>% 
  rename(ID = id)

# check script above to ensure this 230507updated_ML_dataset.Rdata dataset can be replicated
save(test, file = here("output", "230507updated_ML_dataset.Rdata"))
# remove 13, 18, 23, 35, 54, 82 for now as they only have one session, this way the two conditions are completely balanced
test <- test %>% 
  filter(!ID %in% c(13, 18, 23, 35, 54, 82)) 
save(test, file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
```

```{r obselete, for notes}
# grid-search
# need to figure out how to do full grid search, here the combinations are very limited
# maybe don't worry too much about conducting a full grid search as too good parameters here may lead to over-fitting. I think we just need good enough. 
# I found this helpful info: "In practice, a logarithmic grid from to .01 to 1000 is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search." https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html
# I extend it to 15 to be consistent with Hsu et al.'s paper suggestion
# I used RBF kernel by default, also the most recommended one
# simple scaling is done with the e package based on the R documentation 
obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear")
# .6, the above is the best so far, better than cost = 2^(2:4)
obj_test <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
# the above appears to be too over-fitting and leading to much worse performance later on
# check more complex models, not good performance
obj_test <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(2:4), gamma = 2^(-1:1))
obj_test <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15))
# the above one is slightly better, but not as good as signs linear models
obj_test <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(2:4), gamma = 2^(-1:1), tunecontrol = tune.control(cross = nrow(train_set)))
####### to do: based on the paper, may need to follow up with a finer grid search with .25 increments, e.g., 2^3.25. First need to figure out the plot to understand how to select a finer grid range; also the Bayesian method mentioned in the previous link looks cool.

# https://rdrr.io/rforge/e1071/src/R/tune.R
# https://stat.ethz.ch/pipermail/r-help/2005-August/077427.html
# https://stats.stackexchange.com/questions/136274/leave-one-subject-out-cv-method

# sampling method: default is 10-fold cross validation, here I use leave one out with tune.control and this approach is more computationally costly, yet with more data to train
# article discussing the number of fold to select https://cran.r-project.org/web/packages/cvms/vignettes/picking_the_number_of_folds_for_cross-validation.html
# "(on average) the prediction error should be lower with a larger k... A higher number of folds means training a lot more models, which can be computationally heavy and time-consuming. So finding a lower k that yields a similar prediction error most of the time, can be very useful. "

summary(obj)
plot(obj)

# (The dependent variable, Type, has column number 10. cost is a general penalizing parameter for C-classification and gamma is the radial basis function specific kernel parameter.)
# use the best parameter to train the whole training set
# the more complex models above label all the cases as 2, hence ineffective
svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = 0.0009765625, kernel = "linear", probability = TRUE)
# .6
# grep("sleep_deprivation", colnames(train_set))
svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = 0.001953125, kernel = "linear")
# .58 (loov may lead to overfitting, this result performed slightly worse than 10-fold)

svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = 4, gamma = .5)
# always assign 2
svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = 1, gamma = 0.0009765625, probability = TRUE)
# .56
# test
svm.pred <- predict(svm.model, test_set[,-1])
table(pred = svm.pred, true = test_set[,1])
classAgreement(table(pred = svm.pred, true = test_set[,1]))
svm.model$probA
# 0.7171074
svm.model$probB
# 0.01577696

agreement <- svm.pred == test_set[,1]
table(agreement)
prop.table(table(agreement))
# FALSE  TRUE 
#   20    30

# https://stackoverflow.com/questions/62211003/r-studio-svm-classagreement-how-to
# https://rdrr.io/rforge/e1071/man/classAgreement.html

# data visualization
# https://www.statology.org/plot-svm-in-r/
```
## To be edited full dataset SVM
### linear model, 10 fold per k
```{r full dataset: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)

for (i in 1:10) {
  split <- splitTools::partition(test$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- test[split$train,]
  test_set <- test[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear")
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE)# .58
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 
  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table
}
full_train_test_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results)
sink(here("output",'full_train_test_results.csv'))
print(full_train_test_results)
sink()
```
### linear model, LOO
```{r full dataset LOO: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(test$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- test[split$train,]
  test_set <- test[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE, tunecontrol = tune.control(cross = nrow(train_set)))# .58
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 
  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table
}
full_train_test_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results)
sink(here("output",'full_train_test_LOO_results.csv'))
print(full_train_test_LOO_results)
sink()
```
### RBF model, 10 fold per k
```{r full dataset RBF: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(test$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- test[split$train,]
  test_set <- test[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma

# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, gamma = gamma_val, probability = TRUE)# .58
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 
  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table
}
full_train_test_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results)
sink(here("output",'full_train_test_RBF_results.csv'))
print(full_train_test_RBF_results)
sink()
```
### RBF model, LOO
```{r full dataset LOO RBF: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(test$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- test[split$train,]
  test_set <- test[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma

# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, gamma = gamma_val, probability = TRUE, tunecontrol = tune.control(cross = nrow(train_set)))# .58
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 
  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table
}
full_train_test_RBF_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results)
sink(here("output",'full_train_test_RBF_LOO_results.csv'))
print(full_train_test_RBF_LOO_results)
sink()
```


# reduce the impact of intra-individual differences (baseline)
## scaling by dividing the means per person
```{r scale center with mean the 29 dataset}
# load(file = here("output", "230507updated_ML_dataset.Rdata"))
load(file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
View(test)
intra_individual_differences <- test %>% 
  group_by(ID) %>% 
  select(-sleep_deprivation) %>%
  summarise_each(funs(abs)) %>% 
  summarise_each(funs(mean))

View(intra_individual_differences)
test_scaled <- full_join(test, intra_individual_differences, by = "ID")
test_scaled1 <- test_scaled %>% 
  select(-ends_with("_fc.y"))
test_scaled2 <- test_scaled %>% 
  select(-ends_with("_fc.x"))

A <- test_scaled1 %>% 
  select(-c(ID, sleep_deprivation))
B <- test_scaled2 %>% 
  select(-c(ID, sleep_deprivation))
C <- A/B
View(A)
View(B)
View(C)

test_full_scaled <- bind_cols(test_scaled[c(1, 2)], C)
View(test_full_scaled)
save(test_full_scaled, file = here("output", "230520ML_dataset_29_feature_full_balance_center_mean.Rdata"))
```
## scaling by quadramize values
```{r scale quadramize the 29 dataset}
load(file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
View(test)
# check full balance
# test %>% 
#   group_by(ID) %>% 
#   count() %>% 
#   filter(n == 1)

View(compare2)
compare2 <- test %>% 
  select(-sleep_deprivation) %>% 
  group_by(ID) %>% 
  summarise_each(funs(max)) %>% 
  abs() %>% 
  select(-ID)

SD_group <- test %>% 
  filter(sleep_deprivation == 1) %>% 
  arrange(ID)

WR_group <- test %>% 
  filter(sleep_deprivation == 2) %>% 
  arrange(ID)

View(SD_group)
View(WR_group)

View(test)

A <- SD_group %>%
  select(-c(ID, sleep_deprivation))

SD_group2 <- SD_group %>%
  select(-c(ID, sleep_deprivation))

View(A)

B <- WR_group %>%
  select(-c(ID, sleep_deprivation))

WR_group2 <- WR_group %>%
  select(-c(ID, sleep_deprivation))

A[A>0] <- 1
A[A<0] <- 2

B[B>0] <- 3
B[B<0] <- 4
C <- A*B
View(C)
# 3: both positive correlation
# 6: A negative, B positive (SD turn positive correlation to negative correlation)
# 4: A positive, B negative (SD turn negative correlation to positive)
# 8: A negative, B negative 

A_compare2 <- SD_group2/compare2
B_compare2 <- WR_group2/compare2

# mask for 3
C[C!=3] <- 0
C3 <- C
View(C3)
View(A_compare2)
A_compare2_C3 <- A_compare2 * C3
A_compare2_C3[A_compare2_C3==3] <- .75
A_compare2_C3[A_compare2_C3!=.75 & A_compare2_C3!=0] <- .25

View(A_compare2_C3)

B_compare2_C3 <- B_compare2 * C3
B_compare2_C3[B_compare2_C3==3] <- .75
B_compare2_C3[B_compare2_C3!=.75 & B_compare2_C3!=0] <- .25

View(B_compare2_C3)
View(C3)

# mask for 6
C <- A*B
C[C!=6] <- 0
C6 <- C

A_compare2_C6 <- A_compare2 * C6
A_compare2_C6[A_compare2_C6<0] <- -.25
View(A_compare2_C6)

B_compare2_C6 <- B_compare2 * C6
B_compare2_C6[B_compare2_C6>0] <- .25

View(B_compare2_C6)

# mask for 4
C <- A*B
C[C!=4] <- 0
C4 <- C

A_compare2_C4 <- A_compare2 * C4
A_compare2_C4[A_compare2_C4>0] <- .25
View(A_compare2_C4)

B_compare2_C4 <- B_compare2 * C4
B_compare2_C4[B_compare2_C4<0] <- -.25

View(B_compare2_C4)
# mask for 8
C <- A*B
C[C!=8] <- 0
C8 <- C

A_compare2_C8 <- A_compare2 * C8
A_compare2_C8[A_compare2_C8==-8] <- -.25
A_compare2_C8[A_compare2_C8!=-.25 & A_compare2_C8!=0] <- -.75

View(A_compare2_C8)

B_compare2_C8 <- B_compare2 * C8
B_compare2_C8[B_compare2_C8==-8] <- -.25
B_compare2_C8[B_compare2_C8!=-.25 & B_compare2_C8!=0] <- -.75

View(B_compare2_C8)

# merge
A_merge <- A_compare2_C3 + A_compare2_C6 + A_compare2_C4 + A_compare2_C8
B_merge <- B_compare2_C3 + B_compare2_C6 + B_compare2_C4 + B_compare2_C8

SD_group_change <- bind_cols(SD_group[c(1, 2)], A_merge)
WR_group_change <- bind_cols(WR_group[c(1, 2)], B_merge)

group_change <- bind_rows(SD_group_change, WR_group_change)

save(group_change, file = here("output", "230513updated_ML_dataset_group_change_individual_differences.Rdata"))
```
```{r to be edited obselete scale quadramize the 258 dataset}
load(file = here("output", "230511ML_dataset_258_feature_full_balance.Rdata"))

test %>% 
  group_by(ID) %>% 
  count() %>% 
  filter(n == 1)

View(test)
compare2 <- test %>% 
  select(-sleep_deprivation) %>% 
  group_by(ID) %>% 
  summarise_each(funs(max)) %>% 
  abs() %>% 
  select(-ID)

SD_group <- test %>% 
  filter(sleep_deprivation == 1) %>% 
  arrange(ID)

WR_group <- test %>% 
  filter(sleep_deprivation == 2) %>% 
  arrange(ID)

View(A)

A <- SD_group %>%
  select(-c(ID, sleep_deprivation))

B <- WR_group %>%
  select(-c(ID, sleep_deprivation))

A[A>0] <- 1
A[A<0] <- 2

B[B>0] <- 3
B[B<0] <- 4

C <- A*B
# 3: both positive correlation
# 6: A negative, B positive (SD turn positive correlation to negative correlation)
# 4: A positive, B negative (SD turn negative correlation to positive)
# 8: A negative, B negative 

# mask for 3
C[C!=3] <- 0
C3 <- C

A_compare2 <- A/compare2
A_compare2_C3 <- A_compare2 * C3
A_compare2_C3[A_compare2_C3==3] <- 2
A_compare2_C3[A_compare2_C3!=2 & A_compare2_C3!=0] <- 1

View(A_compare2_C3)

B_compare2 <- B/compare2
B_compare2_C3 <- B_compare2 * C3
B_compare2_C3[B_compare2_C3==3] <- 2
B_compare2_C3[B_compare2_C3!=2 & B_compare2_C3!=0] <- 1

View(B_compare2_C3)
View(C3)

# mask for 6
C[C!=6] <- 0
C6 <- C

A_compare2 <- A/compare2
A_compare2_C6 <- A_compare2 * C6
A_compare2_C6[A_compare2_C6<0] <- -1
View(A_compare2_C6)

B_compare2 <- B/compare2
B_compare2_C6 <- B_compare2 * C6
B_compare2_C6[B_compare2_C6>0] <- 1

View(B_compare2_C6)

# mask for 4
C[C!=4] <- 0
C4 <- C

A_compare2 <- A/compare2
A_compare2_C4 <- A_compare2 * C4
A_compare2_C4[A_compare2_C4>0] <- 1
View(A_compare2_C4)

B_compare2 <- B/compare2
B_compare2_C4 <- B_compare2 * C4
B_compare2_C4[B_compare2_C4<0] <- -1

View(B_compare2_C4)
# mask for 8
C[C!=8] <- 0
C8 <- C

A_compare2 <- A/compare2
A_compare2_C8 <- A_compare2 * C8
A_compare2_C8[A_compare2_C8==-8] <- -1
A_compare2_C8[A_compare2_C8!=-1 & A_compare2_C8!=0] <- -2

View(A_compare2_C8)

B_compare2 <- B/compare2
B_compare2_C8 <- B_compare2 * C8
B_compare2_C8[B_compare2_C8==-8] <- -1
B_compare2_C8[B_compare2_C8!=-1 & B_compare2_C8!=0] <- -2

View(B_compare2_C8)

# merge
A_merge <- A_compare2_C3 + A_compare2_C6 + A_compare2_C4 + A_compare2_C8
B_merge <- B_compare2_C3 + B_compare2_C6 + B_compare2_C4 + B_compare2_C8

SD_group_change <- bind_cols(SD_group[c(1, 2)], A_merge)
WR_group_change <- bind_cols(WR_group[c(1, 2)], B_merge)

group_change <- bind_rows(SD_group_change, WR_group_change)

save(group_change, file = here("output", "230511updated_ML_dataset_group_change_individual_differences_258_feature.Rdata"))
```
## obselete scaling by centering
```{r scale centering the well-rested as baseline 29}
load(file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
# test
SD_group <- test %>% 
  filter(sleep_deprivation == 1) %>% 
  arrange(ID)

WR_group <- test %>% 
  filter(sleep_deprivation == 2) %>% 
  arrange(ID)

A <- SD_group %>%
  select(-c(ID, sleep_deprivation))

B <- WR_group %>%
  select(-c(ID, sleep_deprivation))

baseline <- abs(B)
View(baseline)
A_centered <- A/baseline

SD_group_centered <- bind_cols(SD_group[c(1, 2)], A_centered)

group_centered <- bind_rows(SD_group_centered, WR_group)
View(group_centered)
View(SD_group)
View(WR_group)

save(group_centered, file = here("output", "230512ML_dataset_group_centered_full_balance_29.Rdata"))
```

# descriptives
```{r demographic info}
load(file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
View(test)
View(participant)
participant_demo <- participant %>% 
  select(c(sex, age_group, conn_id)) %>% 
  rename(ID = conn_id)
participant_demo <- left_join(test, participant_demo, by = "ID")

participant_demo %>%
  filter(sleep_deprivation == 1) %>% 
  group_by(age_group) %>%
  count()

participant_demo %>%
  filter(sleep_deprivation == 2) %>% 
  group_by(age_group) %>%
  count()
# 34 senior participants, 41 young adult participants, completely balanced for the sleep manipulation sessions

participant_demo %>%
  group_by(age_group, sleep_deprivation, sex) %>%
  count()

# The sample included in this secondary analysis consisted of 75 participants scan data from both visits. There were 34 (18 identified as female) senior adult participants (age 65-75) and 41 (21 identified as female) young adult participants (age 20-30). A total of ten participants data were excluded (one no fMRI data, three due to significant quality assurance index outliers, and six with only one visit. 
# conn ID: 22, 53, 66 those participants' scan data had significant outliers based on QA
# remove conn ID: 13, 18, 23, 35, 54, 82 for now as they only have one session, this way the two conditions are completely balanced
```

# data visualization
```{r plots}
name_list <- list("Reference data" = c("Sleep-deprived", "Well-rested"),
                  "Classified data" = c("Sleep-deprived", "Well-rested"))
# values are from the senior_train_test_young_validate_RBF_LOO_10_repetition_results
# averages of 10 confusion matrixes
confusion_matrix <- matrix(c(7.6, 3.9, 4.4, 8.1),
       nrow = 2,
       ncol = 2,
       dimnames = name_list)

fourfoldplot(confusion_matrix,color = c("grey40", "grey67"))

#caret::confusionMatrix(confusion_matrix)
#F1 score https://wiki.pathmind.com/accuracy-precision-recall-f1
precision <- 7.6/(7.6+4.4)
recall <- 7.6/(7.6+3.9)
F1 <- 2*(precision*recall/(precision+recall))
```

# ML
## senior subset, young validation
### 258 feature
#### 258 quardramize (all chunks below need to be edited)
```{r 258 old participants sub-dataset}
load(file = here("output", "230511ML_dataset_258_feature_full_balance.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(group_change, participant_label, by = "ID")
TEST <- test_old
```
```{r 258 young participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(group_change, participant_label_validation, by = "ID")

#View(participant)
# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# str(fc_s1$sleep_deprivation)
validation_set <- test_young %>% 
  select(-ID) %>% 
  as.data.frame()
```
##### 258 quardramized linear models 
```{r 258 old participants linear: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear")
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_average_fc_linear_100_repetition_results.csv'))
print(senior_train_test_young_validate_results)
sink()
```
```{r 258 old participants LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE, tunecontrol = tune.control(cross = nrow(train_set)))# .58
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_average_fc_linear_LOO_results.csv'))
print(senior_train_test_young_validate_LOO_results)
sink()
```
##### 258 quardramized RBF models
```{r TBR 258 old participants RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
```{r TBR 258 old participants LOO RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
##### 258 quardramized Polynominal models
```{r TBR 258 old participants Polynominal: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_polynominal_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
```{r TBR_DEFER 258 old participants Polynominal LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_polynominal_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
##### 258 quardramized sigmoid models
```{r TBR 258 old participants sigmoid: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_sigmoid_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
```{r TBR_DEFER 258 old participants Sigmoid LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_sigmoid_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
#### 258 averaged fc non-quardramized (all need to be edited)
```{r 258 old participants sub-dataset}
load(file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(test, participant_label, by = "ID")
TEST <- test_old
```
```{r 258 young participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(test, participant_label_validation, by = "ID")

#View(participant)
# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# str(fc_s1$sleep_deprivation)
validation_set <- test_young %>% 
  select(-ID) %>% 
  as.data.frame()
```
##### 258 non-quardramized linear models 
```{r 258 old participants linear: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear")
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_average_fc_linear_100_repetition_results.csv'))
print(senior_train_test_young_validate_results)
sink()
```
```{r 258 old participants LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE, tunecontrol = tune.control(cross = nrow(train_set)))# .58
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_average_fc_linear_LOO_results.csv'))
print(senior_train_test_young_validate_LOO_results)
sink()
```
##### 258 non-quardramized RBF models
```{r TBR 258 old participants RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
```{r TBR 258 old participants LOO RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
##### 258 non-quardramized Polynominal models
```{r TBR 258 old participants Polynominal: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_polynominal_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
```{r TBR_DEFER 258 old participants Polynominal LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_polynominal_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
##### 258 non-quardramized sigmoid models
```{r TBR 258 old participants sigmoid: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_sigmoid_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
```{r TBR_DEFER 258 old participants Sigmoid LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_sigmoid_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```




#### 258 centering (obselete)
```{r updated old participants sub-dataset}
load(file = here("output", "230511ML_dataset_258_feature_full_balance.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(group_change, participant_label, by = "ID")
TEST <- test_old
```
```{r updated young participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(group_change, participant_label_validation, by = "ID")

#View(participant)
# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# str(fc_s1$sleep_deprivation)
validation_set <- test_young %>% 
  select(-ID) %>% 
  as.data.frame()
```
```{r updated old participants RBF 100 rep: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = FALSE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, scale = FALSE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230511_updated_senior_train_test_young_validate_RBF_results_100_repetition.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```

### 29 feature
#### 29 quardramize 
```{r 29 old participants sub-dataset}
load(file = here("output", "230513updated_ML_dataset_group_change_individual_differences.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(group_change, participant_label, by = "ID")
TEST <- test_old
```
```{r 29 young participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(group_change, participant_label_validation, by = "ID")

#View(participant)
# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# str(fc_s1$sleep_deprivation)
validation_set <- test_young %>% 
  select(-ID) %>% 
  as.data.frame()
```
##### 29 quardramize RBF models
```{r 29 old participants RBF 100 rep: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = TRUE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, scale = TRUE, probability = TRUE, decision.values = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1], decision.values = TRUE, probability = TRUE)

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230513_senior_train_test_young_validate_RBF_results_100_repetition.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
```{r 29 old participants RBF LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = TRUE, tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230529_updated_senior_train_test_young_validate_RBF_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_LOO_results)
sink()
```
```{r 29 old participants RBF LOO manual loop: tune, train, test, validation, forced_choice}
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = TRUE, tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)

# test
  svm.pred <- predict(svm.model, test_set[,-1], probability = TRUE, decision.values = TRUE)

  A <- train_set_ID$ID
  B <- test_set_ID$ID
  C <- svm.pred
  result_names <- attributes(C)$names
  result_decision <- attributes(C)$decision.values
  result_probability <- attributes(C)$probabilities

  test <- c(A, B, C, result_names, result_decision, result_probability)
  sink(here("output",'test.csv'))
  print(test)
  sink()
# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230511_updated_senior_train_test_young_validate_RBF_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_LOO_results)
sink()
```

##### 29 quardramize linear models
```{r 29 old participants linear: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results_linear <- vector(mode = "list", length = 100)
acc_table_results_linear <- vector(mode = "list", length = 100)
acc_stats_results_linear <- vector(mode = "list", length = 100)
acc_probA_results_linear <- vector(mode = "list", length = 100)
acc_probB_results_linear <- vector(mode = "list", length = 100)
acc_agreement_results_linear <- vector(mode = "list", length = 100)
acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
acc_prop_table_results_linear <- vector(mode = "list", length = 100)

validation_acc_results_linear <- vector(mode = "list", length = 100)
validation_acc_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_stats_results_linear <- vector(mode = "list", length = 100)
validation_acc_probA_results_linear <- vector(mode = "list", length = 100)
validation_acc_probB_results_linear <- vector(mode = "list", length = 100)
validation_acc_agreement_results_linear <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_prop_table_results_linear <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_linear <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), kernel = "linear", scale = FALSE)
  cost_val <- obj_linear$best.parameters$cost
# train the model with the whole train_set and optimized parameters
  svm.model_linear <- svm(sleep_deprivation~., data = train_set, cost = cost_val, kernel = "linear")
# plotting
# View(train_set)
# plot(svm.model_linear, train_set, w_DMN_fc ~ w_DAN_fc)
# plot(svm.model_polynomial, train_set, w_DMN_fc ~ w_DAN_fc)

# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_linear)
# test
  svm.pred_linear <- predict(svm.model_linear, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_linear <- mean(svm.pred_linear == test_set[,1])
  acc_table_linear <- table(pred = svm.pred_linear, true = test_set[,1])
  acc_stats_linear <- classAgreement(table(pred = svm.pred_linear, true = test_set[,1]))
  acc_probA_linear <- svm.model_linear$probA
# 
  acc_probB_linear <- svm.model_linear$probB
# 

  acc_agreement_linear <- svm.pred_linear == test_set[,1]
  acc_agreement_table_linear <- table(acc_agreement_linear)
  acc_prop_table_linear <- prop.table(table(acc_agreement_linear))
# FALSE  TRUE 
# store test results
  acc_results_linear[[i]] <- accuracy_linear
  acc_table_results_linear[[i]] <- acc_table_linear
  acc_stats_results_linear[[i]] <- acc_stats_linear
  acc_probA_results_linear[[i]] <- acc_probA_linear
  acc_probB_results_linear[[i]] <- acc_probB_linear
  acc_agreement_results_linear[[i]] <- acc_agreement_linear
  acc_agreement_table_results_linear[[i]] <- acc_agreement_table_linear
  acc_prop_table_results_linear[[i]] <- acc_prop_table_linear

# validation
  svm.pred_validation_linear <- predict(svm.model_linear, validation_set[,-1])
  validation_accuracy_linear <- mean(svm.pred_validation_linear == validation_set[,1])
  validation_acc_table_linear <- table(pred = svm.pred_validation_linear, true = validation_set[,1])
  validation_acc_stats_linear <- classAgreement(table(pred = svm.pred_validation_linear, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_linear <- svm.model_linear$probA
#  
  validation_acc_probB_linear <- svm.model_linear$probB
#
  validation_acc_agreement_linear <- svm.pred_validation_linear == validation_set[,1]
  validation_acc_agreement_table_linear <- table(validation_acc_agreement_linear)
  validation_acc_prop_table_linear <- prop.table(table(validation_acc_agreement_linear))
  
  validation_acc_results_linear[[i]] <- validation_accuracy_linear
  validation_acc_table_results_linear[[i]] <- validation_acc_table_linear
  validation_acc_stats_results_linear[[i]] <- validation_acc_stats_linear
  validation_acc_probA_results_linear[[i]] <- validation_acc_probA_linear
  validation_acc_probB_results_linear[[i]] <- validation_acc_probB_linear
  validation_acc_agreement_results_linear[[i]] <- validation_acc_agreement_linear
  validation_acc_agreement_table_results_linear[[i]] <- validation_acc_agreement_table_linear
  validation_acc_prop_table_results_linear[[i]] <- validation_acc_prop_table_linear
}
senior_train_test_young_validate_linear_results <- c(acc_results_linear, acc_table_results_linear, acc_stats_results_linear, acc_probA_results_linear, acc_probB_results_linear, acc_agreement_results_linear, acc_agreement_table_results_linear, acc_prop_table_results_linear, validation_acc_results_linear, validation_acc_table_results_linear, validation_acc_stats_results_linear, validation_acc_probA_results_linear, validation_acc_probB_results_linear, validation_acc_agreement_results_linear, validation_acc_agreement_table_results_linear, validation_acc_prop_table_results_linear)
sink(here("output",'230513_senior_train_test_young_validate_linear_100_repetition_results.csv'))
print(senior_train_test_young_validate_linear_results)
sink()
```
```{r 29 old participants linear LOO: tune, train, test, validation}
acc_results_linear <- vector(mode = "list", length = 10)
acc_table_results_linear <- vector(mode = "list", length = 10)
acc_stats_results_linear <- vector(mode = "list", length = 10)
acc_probA_results_linear <- vector(mode = "list", length = 10)
acc_probB_results_linear <- vector(mode = "list", length = 100)
acc_agreement_results_linear <- vector(mode = "list", length = 10)
acc_agreement_table_results_linear <- vector(mode = "list", length = 10)
acc_prop_table_results_linear <- vector(mode = "list", length = 10)

validation_acc_results_linear <- vector(mode = "list", length = 10)
validation_acc_table_results_linear <- vector(mode = "list", length = 10)
validation_acc_stats_results_linear <- vector(mode = "list", length = 10)
validation_acc_probA_results_linear <- vector(mode = "list", length = 10)
validation_acc_probB_results_linear <- vector(mode = "list", length = 10)
validation_acc_agreement_results_linear <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results_linear <- vector(mode = "list", length = 10)
validation_acc_prop_table_results_linear <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_linear <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj_linear$best.parameters$cost
# train the model with the whole train_set and optimized parameters
  svm.model_linear <- svm(sleep_deprivation~., data = train_set, cost = cost_val, kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
# plotting
# View(train_set)
# plot(svm.model_linear, train_set, w_DMN_fc ~ w_DAN_fc)
# plot(svm.model_polynomial, train_set, w_DMN_fc ~ w_DAN_fc)

# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_linear)
# test
  svm.pred_linear <- predict(svm.model_linear, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_linear <- mean(svm.pred_linear == test_set[,1])
  acc_table_linear <- table(pred = svm.pred_linear, true = test_set[,1])
  acc_stats_linear <- classAgreement(table(pred = svm.pred_linear, true = test_set[,1]))
  acc_probA_linear <- svm.model_linear$probA
# 
  acc_probB_linear <- svm.model_linear$probB
# 

  acc_agreement_linear <- svm.pred_linear == test_set[,1]
  acc_agreement_table_linear <- table(acc_agreement_linear)
  acc_prop_table_linear <- prop.table(table(acc_agreement_linear))
# FALSE  TRUE 
# store test results
  acc_results_linear[[i]] <- accuracy_linear
  acc_table_results_linear[[i]] <- acc_table_linear
  acc_stats_results_linear[[i]] <- acc_stats_linear
  acc_probA_results_linear[[i]] <- acc_probA_linear
  acc_probB_results_linear[[i]] <- acc_probB_linear
  acc_agreement_results_linear[[i]] <- acc_agreement_linear
  acc_agreement_table_results_linear[[i]] <- acc_agreement_table_linear
  acc_prop_table_results_linear[[i]] <- acc_prop_table_linear

# validation
  svm.pred_validation_linear <- predict(svm.model_linear, validation_set[,-1])
  validation_accuracy_linear <- mean(svm.pred_validation_linear == validation_set[,1])
  validation_acc_table_linear <- table(pred = svm.pred_validation_linear, true = validation_set[,1])
  validation_acc_stats_linear <- classAgreement(table(pred = svm.pred_validation_linear, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_linear <- svm.model_linear$probA
#  
  validation_acc_probB_linear <- svm.model_linear$probB
#
  validation_acc_agreement_linear <- svm.pred_validation_linear == validation_set[,1]
  validation_acc_agreement_table_linear <- table(validation_acc_agreement_linear)
  validation_acc_prop_table_linear <- prop.table(table(validation_acc_agreement_linear))
  
  validation_acc_results_linear[[i]] <- validation_accuracy_linear
  validation_acc_table_results_linear[[i]] <- validation_acc_table_linear
  validation_acc_stats_results_linear[[i]] <- validation_acc_stats_linear
  validation_acc_probA_results_linear[[i]] <- validation_acc_probA_linear
  validation_acc_probB_results_linear[[i]] <- validation_acc_probB_linear
  validation_acc_agreement_results_linear[[i]] <- validation_acc_agreement_linear
  validation_acc_agreement_table_results_linear[[i]] <- validation_acc_agreement_table_linear
  validation_acc_prop_table_results_linear[[i]] <- validation_acc_prop_table_linear
}
senior_train_test_young_validate_linear_results <- c(acc_results_linear, acc_table_results_linear, acc_stats_results_linear, acc_probA_results_linear, acc_probB_results_linear, acc_agreement_results_linear, acc_agreement_table_results_linear, acc_prop_table_results_linear, validation_acc_results_linear, validation_acc_table_results_linear, validation_acc_stats_results_linear, validation_acc_probA_results_linear, validation_acc_probB_results_linear, validation_acc_agreement_results_linear, validation_acc_agreement_table_results_linear, validation_acc_prop_table_results_linear)
sink(here("output",'230513_senior_train_test_young_validate_linear_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_linear_results)
sink()
```
##### 29 quardramize Polynominal models TAKING TOO MUCH TIME TO RUN AND WARNING ABOUT REACHING MAX NUM OF ITERATIONS
```{r TBR 29 old participants polynomial: tune, train, test, validation}
acc_results_polynomial <- vector(mode = "list", length = 100)
acc_table_results_polynomial <- vector(mode = "list", length = 100)
acc_stats_results_polynomial <- vector(mode = "list", length = 100)
acc_probA_results_polynomial <- vector(mode = "list", length = 100)
acc_probB_results_polynomial <- vector(mode = "list", length = 100)
acc_agreement_results_polynomial <- vector(mode = "list", length = 100)
acc_agreement_table_results_polynomial <- vector(mode = "list", length = 100)
acc_prop_table_results_polynomial <- vector(mode = "list", length = 100)

validation_acc_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_table_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_stats_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_probA_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_probB_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_agreement_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_prop_table_results_polynomial <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_polynomial <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial")
  cost_val <- obj_polynomial$best.parameters$cost
  gamma_val <- obj_polynomial$best.parameters$gamma
  coef0_val <- obj_polynomial$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model_polynomial <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial")
# grep("sleep_deprivation", colnames(train_set))
#summary(svm.model_polynomial)
# test
  svm.pred_polynomial <- predict(svm.model_polynomial, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_polynomial <- mean(svm.pred_polynomial == test_set[,1])
  acc_table_polynomial <- table(pred = svm.pred_polynomial, true = test_set[,1])
  acc_stats_polynomial <- classAgreement(table(pred = svm.pred_polynomial, true = test_set[,1]))
  acc_probA_polynomial <- svm.model_polynomial$probA
# 
  acc_probB_polynomial <- svm.model_polynomial$probB
# 

  acc_agreement_polynomial <- svm.pred_polynomial == test_set[,1]
  acc_agreement_table_polynomial <- table(acc_agreement_polynomial)
  acc_prop_table_polynomial <- prop.table(table(acc_agreement_polynomial))
# FALSE  TRUE 
# store test results
  acc_results_polynomial[[i]] <- accuracy_polynomial
  acc_table_results_polynomial[[i]] <- acc_table_polynomial
  acc_stats_results_polynomial[[i]] <- acc_stats_polynomial
  acc_probA_results_polynomial[[i]] <- acc_probA_polynomial
  acc_probB_results_polynomial[[i]] <- acc_probB_polynomial
  acc_agreement_results_polynomial[[i]] <- acc_agreement_polynomial
  acc_agreement_table_results_polynomial[[i]] <- acc_agreement_table_polynomial
  acc_prop_table_results_polynomial[[i]] <- acc_prop_table_polynomial

# validation
  svm.pred_validation_polynomial <- predict(svm.model_polynomial, validation_set[,-1])
  validation_accuracy_polynomial <- mean(svm.pred_validation_polynomial == validation_set[,1])
  validation_acc_table_polynomial <- table(pred = svm.pred_validation_polynomial, true = validation_set[,1])
  validation_acc_stats_polynomial <- classAgreement(table(pred = svm.pred_validation_polynomial, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_polynomial <- svm.model_polynomial$probA
#  
  validation_acc_probB_polynomial <- svm.model_polynomial$probB
#
  validation_acc_agreement_polynomial <- svm.pred_validation_polynomial == validation_set[,1]
  validation_acc_agreement_table_polynomial <- table(validation_acc_agreement_polynomial)
  validation_acc_prop_table_polynomial <- prop.table(table(validation_acc_agreement_polynomial))
  
  validation_acc_results_polynomial[[i]] <- validation_accuracy_polynomial
  validation_acc_table_results_polynomial[[i]] <- validation_acc_table_polynomial
  validation_acc_stats_results_polynomial[[i]] <- validation_acc_stats_polynomial
  validation_acc_probA_results_polynomial[[i]] <- validation_acc_probA_polynomial
  validation_acc_probB_results_polynomial[[i]] <- validation_acc_probB_polynomial
  validation_acc_agreement_results_polynomial[[i]] <- validation_acc_agreement_polynomial
  validation_acc_agreement_table_results_polynomial[[i]] <- validation_acc_agreement_table_polynomial
  validation_acc_prop_table_results_polynomial[[i]] <- validation_acc_prop_table_polynomial
}
senior_train_test_young_validate_polynomial_results <- c(acc_results_polynomial, acc_table_results_polynomial, acc_stats_results_polynomial, acc_probA_results_polynomial, acc_probB_results_polynomial, acc_agreement_results_polynomial, acc_agreement_table_results_polynomial, acc_prop_table_results_polynomial, validation_acc_results_polynomial, validation_acc_table_results_polynomial, validation_acc_stats_results_polynomial, validation_acc_probA_results_polynomial, validation_acc_probB_results_polynomial, validation_acc_agreement_results_polynomial, validation_acc_agreement_table_results_polynomial, validation_acc_prop_table_results_polynomial)
sink(here("output",'230515_senior_train_test_young_validate_polynominal_100_repetition_results.csv'))
print(senior_train_test_young_validate_polynomial_results)
sink()
```
```{r TBR 29 old participants polynomial LOO: tune, train, test, validation}
acc_results_polynomial <- vector(mode = "list", length = 10)
acc_table_results_polynomial <- vector(mode = "list", length = 10)
acc_stats_results_polynomial <- vector(mode = "list", length = 10)
acc_probA_results_polynomial <- vector(mode = "list", length = 10)
acc_probB_results_polynomial <- vector(mode = "list", length = 10)
acc_agreement_results_polynomial <- vector(mode = "list", length = 10)
acc_agreement_table_results_polynomial <- vector(mode = "list", length = 10)
acc_prop_table_results_polynomial <- vector(mode = "list", length = 10)

validation_acc_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_table_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_stats_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_probA_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_probB_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_agreement_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_prop_table_results_polynomial <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj_polynomial <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj_polynomial$best.parameters$cost
  gamma_val <- obj_polynomial$best.parameters$gamma
  coef0_val <- obj_polynomial$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model_polynomial <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_polynomial)
# test
  svm.pred_polynomial <- predict(svm.model_polynomial, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_polynomial <- mean(svm.pred_polynomial == test_set[,1])
  acc_table_polynomial <- table(pred = svm.pred_polynomial, true = test_set[,1])
  acc_stats_polynomial <- classAgreement(table(pred = svm.pred_polynomial, true = test_set[,1]))
  acc_probA_polynomial <- svm.model_polynomial$probA
# 
  acc_probB_polynomial <- svm.model_polynomial$probB
# 

  acc_agreement_polynomial <- svm.pred_polynomial == test_set[,1]
  acc_agreement_table_polynomial <- table(acc_agreement_polynomial)
  acc_prop_table_polynomial <- prop.table(table(acc_agreement_polynomial))
# FALSE  TRUE 
# store test results
  acc_results_polynomial[[i]] <- accuracy_polynomial
  acc_table_results_polynomial[[i]] <- acc_table_polynomial
  acc_stats_results_polynomial[[i]] <- acc_stats_polynomial
  acc_probA_results_polynomial[[i]] <- acc_probA_polynomial
  acc_probB_results_polynomial[[i]] <- acc_probB_polynomial
  acc_agreement_results_polynomial[[i]] <- acc_agreement_polynomial
  acc_agreement_table_results_polynomial[[i]] <- acc_agreement_table_polynomial
  acc_prop_table_results_polynomial[[i]] <- acc_prop_table_polynomial

# validation
  svm.pred_validation_polynomial <- predict(svm.model_polynomial, validation_set[,-1])
  validation_accuracy_polynomial <- mean(svm.pred_validation_polynomial == validation_set[,1])
  validation_acc_table_polynomial <- table(pred = svm.pred_validation_polynomial, true = validation_set[,1])
  validation_acc_stats_polynomial <- classAgreement(table(pred = svm.pred_validation_polynomial, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_polynomial <- svm.model_polynomial$probA
#  
  validation_acc_probB_polynomial <- svm.model_polynomial$probB
#
  validation_acc_agreement_polynomial <- svm.pred_validation_polynomial == validation_set[,1]
  validation_acc_agreement_table_polynomial <- table(validation_acc_agreement_polynomial)
  validation_acc_prop_table_polynomial <- prop.table(table(validation_acc_agreement_polynomial))
  
  validation_acc_results_polynomial[[i]] <- validation_accuracy_polynomial
  validation_acc_table_results_polynomial[[i]] <- validation_acc_table_polynomial
  validation_acc_stats_results_polynomial[[i]] <- validation_acc_stats_polynomial
  validation_acc_probA_results_polynomial[[i]] <- validation_acc_probA_polynomial
  validation_acc_probB_results_polynomial[[i]] <- validation_acc_probB_polynomial
  validation_acc_agreement_results_polynomial[[i]] <- validation_acc_agreement_polynomial
  validation_acc_agreement_table_results_polynomial[[i]] <- validation_acc_agreement_table_polynomial
  validation_acc_prop_table_results_polynomial[[i]] <- validation_acc_prop_table_polynomial
}
senior_train_test_young_validate_polynominal_LOO_10_repetition_results <- c(acc_results_polynomial, acc_table_results_polynomial, acc_stats_results_polynomial, acc_probA_results_polynomial, acc_probB_results_polynomial, acc_agreement_results_polynomial, acc_agreement_table_results_polynomial, acc_prop_table_results_polynomial, validation_acc_results_polynomial, validation_acc_table_results_polynomial, validation_acc_stats_results_polynomial, validation_acc_probA_results_polynomial, validation_acc_probB_results_polynomial, validation_acc_agreement_results_polynomial, validation_acc_agreement_table_results_polynomial, validation_acc_prop_table_results_polynomial)
sink(here("output",'230515_senior_train_test_young_validate_polynominal_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_LOO_10_repetition_results)
sink()
```
##### 29 quardramize sigmoid models TAKING TOO MUCH TIME TO RUN AND WARNING ABOUT REACHING MAX NUM OF ITERATIONS
```{r TBR 29 old participants sigmoid: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results_sigmoid <- vector(mode = "list", length = 100)
acc_table_results_sigmoid <- vector(mode = "list", length = 100)
acc_stats_results_sigmoid <- vector(mode = "list", length = 100)
acc_probA_results_sigmoid <- vector(mode = "list", length = 100)
acc_probB_results_sigmoid <- vector(mode = "list", length = 100)
acc_agreement_results_sigmoid <- vector(mode = "list", length = 100)
acc_agreement_table_results_sigmoid <- vector(mode = "list", length = 100)
acc_prop_table_results_sigmoid <- vector(mode = "list", length = 100)

validation_acc_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_table_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_stats_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_probA_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_probB_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_agreement_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_prop_table_results_sigmoid <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_sigmoid <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid")
  cost_val <- obj_sigmoid$best.parameters$cost
  gamma_val <- obj_sigmoid$best.parameters$gamma
  coef0_val <- obj_sigmoid$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model_sigmoid <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid")
# grep("sleep_deprivation", colnames(train_set))
#summary(svm.model_sigmoid)
# test
  svm.pred_sigmoid <- predict(svm.model_sigmoid, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_sigmoid <- mean(svm.pred_sigmoid == test_set[,1])
  acc_table_sigmoid <- table(pred = svm.pred_sigmoid, true = test_set[,1])
  acc_stats_sigmoid <- classAgreement(table(pred = svm.pred_sigmoid, true = test_set[,1]))
  acc_probA_sigmoid <- svm.model_sigmoid$probA
# 
  acc_probB_sigmoid <- svm.model_sigmoid$probB
# 

  acc_agreement_sigmoid <- svm.pred_sigmoid == test_set[,1]
  acc_agreement_table_sigmoid <- table(acc_agreement_sigmoid)
  acc_prop_table_sigmoid <- prop.table(table(acc_agreement_sigmoid))
# FALSE  TRUE 
# store test results
  acc_results_sigmoid[[i]] <- accuracy_sigmoid
  acc_table_results_sigmoid[[i]] <- acc_table_sigmoid
  acc_stats_results_sigmoid[[i]] <- acc_stats_sigmoid
  acc_probA_results_sigmoid[[i]] <- acc_probA_sigmoid
  acc_probB_results_sigmoid[[i]] <- acc_probB_sigmoid
  acc_agreement_results_sigmoid[[i]] <- acc_agreement_sigmoid
  acc_agreement_table_results_sigmoid[[i]] <- acc_agreement_table_sigmoid
  acc_prop_table_results_sigmoid[[i]] <- acc_prop_table_sigmoid

# validation
  svm.pred_validation_sigmoid <- predict(svm.model_sigmoid, validation_set[,-1])
  validation_accuracy_sigmoid <- mean(svm.pred_validation_sigmoid == validation_set[,1])
  validation_acc_table_sigmoid <- table(pred = svm.pred_validation_sigmoid, true = validation_set[,1])
  validation_acc_stats_sigmoid <- classAgreement(table(pred = svm.pred_validation_sigmoid, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_sigmoid <- svm.model_sigmoid$probA
#  
  validation_acc_probB_sigmoid <- svm.model_sigmoid$probB
#
  validation_acc_agreement_sigmoid <- svm.pred_validation_sigmoid == validation_set[,1]
  validation_acc_agreement_table_sigmoid <- table(validation_acc_agreement_sigmoid)
  validation_acc_prop_table_sigmoid <- prop.table(table(validation_acc_agreement_sigmoid))
  
  validation_acc_results_sigmoid[[i]] <- validation_accuracy_sigmoid
  validation_acc_table_results_sigmoid[[i]] <- validation_acc_table_sigmoid
  validation_acc_stats_results_sigmoid[[i]] <- validation_acc_stats_sigmoid
  validation_acc_probA_results_sigmoid[[i]] <- validation_acc_probA_sigmoid
  validation_acc_probB_results_sigmoid[[i]] <- validation_acc_probB_sigmoid
  validation_acc_agreement_results_sigmoid[[i]] <- validation_acc_agreement_sigmoid
  validation_acc_agreement_table_results_sigmoid[[i]] <- validation_acc_agreement_table_sigmoid
  validation_acc_prop_table_results_sigmoid[[i]] <- validation_acc_prop_table_sigmoid
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results_sigmoid, acc_table_results_sigmoid, acc_stats_results_sigmoid, acc_probA_results_sigmoid, acc_probB_results_sigmoid, acc_agreement_results_sigmoid, acc_agreement_table_results_sigmoid, acc_prop_table_results_sigmoid, validation_acc_results_sigmoid, validation_acc_table_results_sigmoid, validation_acc_stats_results_sigmoid, validation_acc_probA_results_sigmoid, validation_acc_probB_results_sigmoid, validation_acc_agreement_results_sigmoid, validation_acc_agreement_table_results_sigmoid, validation_acc_prop_table_results_sigmoid)
sink(here("output",'230513_updated_senior_train_test_young_validate_sigmoid_100_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
```{r TBR 29 updated old participants sigmoid LOO: tune, train, test, validation}
acc_results_sigmoid <- vector(mode = "list", length = 10)
acc_table_results_sigmoid <- vector(mode = "list", length = 10)
acc_stats_results_sigmoid <- vector(mode = "list", length = 10)
acc_probA_results_sigmoid <- vector(mode = "list", length = 10)
acc_probB_results_sigmoid <- vector(mode = "list", length = 10)
acc_agreement_results_sigmoid <- vector(mode = "list", length = 10)
acc_agreement_table_results_sigmoid <- vector(mode = "list", length = 10)
acc_prop_table_results_sigmoid <- vector(mode = "list", length = 10)

validation_acc_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_table_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_stats_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_probA_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_probB_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_agreement_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_prop_table_results_sigmoid <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj_sigmoid <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj_sigmoid$best.parameters$cost
  gamma_val <- obj_sigmoid$best.parameters$gamma
  coef0_val <- obj_sigmoid$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model_sigmoid <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))
# summary(svm.model_sigmoid)
# test
  svm.pred_sigmoid <- predict(svm.model_sigmoid, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_sigmoid <- mean(svm.pred_sigmoid == test_set[,1])
  acc_table_sigmoid <- table(pred = svm.pred_sigmoid, true = test_set[,1])
  acc_stats_sigmoid <- classAgreement(table(pred = svm.pred_sigmoid, true = test_set[,1]))
  acc_probA_sigmoid <- svm.model_sigmoid$probA
# 
  acc_probB_sigmoid <- svm.model_sigmoid$probB
# 

  acc_agreement_sigmoid <- svm.pred_sigmoid == test_set[,1]
  acc_agreement_table_sigmoid <- table(acc_agreement_sigmoid)
  acc_prop_table_sigmoid <- prop.table(table(acc_agreement_sigmoid))
# FALSE  TRUE 
# store test results
  acc_results_sigmoid[[i]] <- accuracy_sigmoid
  acc_table_results_sigmoid[[i]] <- acc_table_sigmoid
  acc_stats_results_sigmoid[[i]] <- acc_stats_sigmoid
  acc_probA_results_sigmoid[[i]] <- acc_probA_sigmoid
  acc_probB_results_sigmoid[[i]] <- acc_probB_sigmoid
  acc_agreement_results_sigmoid[[i]] <- acc_agreement_sigmoid
  acc_agreement_table_results_sigmoid[[i]] <- acc_agreement_table_sigmoid
  acc_prop_table_results_sigmoid[[i]] <- acc_prop_table_sigmoid

# validation
  svm.pred_validation_sigmoid <- predict(svm.model_sigmoid, validation_set[,-1])
  validation_accuracy_sigmoid <- mean(svm.pred_validation_sigmoid == validation_set[,1])
  validation_acc_table_sigmoid <- table(pred = svm.pred_validation_sigmoid, true = validation_set[,1])
  validation_acc_stats_sigmoid <- classAgreement(table(pred = svm.pred_validation_sigmoid, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_sigmoid <- svm.model_sigmoid$probA
#  
  validation_acc_probB_sigmoid <- svm.model_sigmoid$probB
#
  validation_acc_agreement_sigmoid <- svm.pred_validation_sigmoid == validation_set[,1]
  validation_acc_agreement_table_sigmoid <- table(validation_acc_agreement_sigmoid)
  validation_acc_prop_table_sigmoid <- prop.table(table(validation_acc_agreement_sigmoid))
  
  validation_acc_results_sigmoid[[i]] <- validation_accuracy_sigmoid
  validation_acc_table_results_sigmoid[[i]] <- validation_acc_table_sigmoid
  validation_acc_stats_results_sigmoid[[i]] <- validation_acc_stats_sigmoid
  validation_acc_probA_results_sigmoid[[i]] <- validation_acc_probA_sigmoid
  validation_acc_probB_results_sigmoid[[i]] <- validation_acc_probB_sigmoid
  validation_acc_agreement_results_sigmoid[[i]] <- validation_acc_agreement_sigmoid
  validation_acc_agreement_table_results_sigmoid[[i]] <- validation_acc_agreement_table_sigmoid
  validation_acc_prop_table_results_sigmoid[[i]] <- validation_acc_prop_table_sigmoid
}
senior_train_test_young_validate_sigmoid_LOO_10_repetition_results <- c(acc_results_sigmoid, acc_table_results_sigmoid, acc_stats_results_sigmoid, acc_probA_results_sigmoid, acc_probB_results_sigmoid, acc_agreement_results_sigmoid, acc_agreement_table_results_sigmoid, acc_prop_table_results_sigmoid, validation_acc_results_sigmoid, validation_acc_table_results_sigmoid, validation_acc_stats_results_sigmoid, validation_acc_probA_results_sigmoid, validation_acc_probB_results_sigmoid, validation_acc_agreement_results_sigmoid, validation_acc_agreement_table_results_sigmoid, validation_acc_prop_table_results_sigmoid)
sink(here("output",'230515_senior_train_test_young_validate_sigmoid_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_LOO_10_repetition_results)
sink()
```

#### 29 averaged fc non-quardramized
```{r 29 old participants sub-dataset}
load(file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(test, participant_label, by = "ID")
TEST <- test_old
```
```{r 29 young participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(test, participant_label_validation, by = "ID")

#View(participant)
# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# str(fc_s1$sleep_deprivation)
validation_set <- test_young %>% 
  select(-ID) %>% 
  as.data.frame()
```
##### 29 non-quardramized linear models 
```{r 29 old participants linear: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear")
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_average_fc_linear_100_repetition_results.csv'))
print(senior_train_test_young_validate_results)
sink()
```
```{r 29 old participants LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE, tunecontrol = tune.control(cross = nrow(train_set)))# .58
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_average_fc_linear_LOO_results.csv'))
print(senior_train_test_young_validate_LOO_results)
sink()
```
##### 29 non-quardramized RBF models
```{r old participants RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
```{r old participants LOO RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
##### 29 non-quardramized Polynominal models TAKING TOO MUCH TIME TO RUN AND WARNING ABOUT REACHING MAX NUM OF ITERATIONS
```{r TBR old participants Polynominal: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_polynominal_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
```{r TBR_DEFER old participants Polynominal LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_polynominal_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
##### 29 non-quardramized sigmoid models TAKING TOO MUCH TIME TO RUN AND WARNING ABOUT REACHING MAX NUM OF ITERATIONS
```{r TBR old participants sigmoid: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_sigmoid_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
```{r TBR_DEFER old participants Sigmoid LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_sigmoid_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```

#### 29 centering with mean
```{r 29 old participants sub-dataset}
load(file = here("output", "230520ML_dataset_29_feature_full_balance_center_mean.Rdata"))
# test_full_scaled
participant_label <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(test_full_scaled, participant_label, by = "ID")
TEST <- test_old
```
```{r 29 young participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(test_full_scaled, participant_label_validation, by = "ID")

#View(participant)
# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# str(fc_s1$sleep_deprivation)
validation_set <- test_young %>% 
  select(-ID) %>% 
  as.data.frame()
```
##### 29 centering RBF models
```{r 29 old participants RBF 100 rep: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = TRUE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, scale = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230521_center_mean_senior_train_test_young_validate_RBF_results_100_repetition.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
```{r 29 old participants RBF LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = FALSE, tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230521_center_mean_senior_train_test_young_validate_RBF_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_LOO_results)
sink()
```
##### 29 centering linear models
```{r 29 old participants linear: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results_linear <- vector(mode = "list", length = 100)
acc_table_results_linear <- vector(mode = "list", length = 100)
acc_stats_results_linear <- vector(mode = "list", length = 100)
acc_probA_results_linear <- vector(mode = "list", length = 100)
acc_probB_results_linear <- vector(mode = "list", length = 100)
acc_agreement_results_linear <- vector(mode = "list", length = 100)
acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
acc_prop_table_results_linear <- vector(mode = "list", length = 100)

validation_acc_results_linear <- vector(mode = "list", length = 100)
validation_acc_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_stats_results_linear <- vector(mode = "list", length = 100)
validation_acc_probA_results_linear <- vector(mode = "list", length = 100)
validation_acc_probB_results_linear <- vector(mode = "list", length = 100)
validation_acc_agreement_results_linear <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_prop_table_results_linear <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj_linear <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), kernel = "linear", scale = FALSE)
  cost_val <- obj_linear$best.parameters$cost
# train the model with the whole train_set and optimized parameters
  svm.model_linear <- svm(sleep_deprivation~., data = train_set, cost = cost_val, kernel = "linear")

# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_linear)
# test
  svm.pred_linear <- predict(svm.model_linear, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_linear <- mean(svm.pred_linear == test_set[,1])
  acc_table_linear <- table(pred = svm.pred_linear, true = test_set[,1])
  acc_stats_linear <- classAgreement(table(pred = svm.pred_linear, true = test_set[,1]))
  acc_probA_linear <- svm.model_linear$probA
# 
  acc_probB_linear <- svm.model_linear$probB
# 

  acc_agreement_linear <- svm.pred_linear == test_set[,1]
  acc_agreement_table_linear <- table(acc_agreement_linear)
  acc_prop_table_linear <- prop.table(table(acc_agreement_linear))
# FALSE  TRUE 
# store test results
  acc_results_linear[[i]] <- accuracy_linear
  acc_table_results_linear[[i]] <- acc_table_linear
  acc_stats_results_linear[[i]] <- acc_stats_linear
  acc_probA_results_linear[[i]] <- acc_probA_linear
  acc_probB_results_linear[[i]] <- acc_probB_linear
  acc_agreement_results_linear[[i]] <- acc_agreement_linear
  acc_agreement_table_results_linear[[i]] <- acc_agreement_table_linear
  acc_prop_table_results_linear[[i]] <- acc_prop_table_linear

# validation
  svm.pred_validation_linear <- predict(svm.model_linear, validation_set[,-1])
  validation_accuracy_linear <- mean(svm.pred_validation_linear == validation_set[,1])
  validation_acc_table_linear <- table(pred = svm.pred_validation_linear, true = validation_set[,1])
  validation_acc_stats_linear <- classAgreement(table(pred = svm.pred_validation_linear, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_linear <- svm.model_linear$probA
#  
  validation_acc_probB_linear <- svm.model_linear$probB
#
  validation_acc_agreement_linear <- svm.pred_validation_linear == validation_set[,1]
  validation_acc_agreement_table_linear <- table(validation_acc_agreement_linear)
  validation_acc_prop_table_linear <- prop.table(table(validation_acc_agreement_linear))
  
  validation_acc_results_linear[[i]] <- validation_accuracy_linear
  validation_acc_table_results_linear[[i]] <- validation_acc_table_linear
  validation_acc_stats_results_linear[[i]] <- validation_acc_stats_linear
  validation_acc_probA_results_linear[[i]] <- validation_acc_probA_linear
  validation_acc_probB_results_linear[[i]] <- validation_acc_probB_linear
  validation_acc_agreement_results_linear[[i]] <- validation_acc_agreement_linear
  validation_acc_agreement_table_results_linear[[i]] <- validation_acc_agreement_table_linear
  validation_acc_prop_table_results_linear[[i]] <- validation_acc_prop_table_linear
}
senior_train_test_young_validate_linear_results <- c(acc_results_linear, acc_table_results_linear, acc_stats_results_linear, acc_probA_results_linear, acc_probB_results_linear, acc_agreement_results_linear, acc_agreement_table_results_linear, acc_prop_table_results_linear, validation_acc_results_linear, validation_acc_table_results_linear, validation_acc_stats_results_linear, validation_acc_probA_results_linear, validation_acc_probB_results_linear, validation_acc_agreement_results_linear, validation_acc_agreement_table_results_linear, validation_acc_prop_table_results_linear)
sink(here("output",'230521_center_mean_senior_train_test_young_validate_linear_100_repetition_results.csv'))
print(senior_train_test_young_validate_linear_results)
sink()
```
```{r 29 old participants linear LOO: tune, train, test, validation}
acc_results_linear <- vector(mode = "list", length = 10)
acc_table_results_linear <- vector(mode = "list", length = 10)
acc_stats_results_linear <- vector(mode = "list", length = 10)
acc_probA_results_linear <- vector(mode = "list", length = 10)
acc_probB_results_linear <- vector(mode = "list", length = 100)
acc_agreement_results_linear <- vector(mode = "list", length = 10)
acc_agreement_table_results_linear <- vector(mode = "list", length = 10)
acc_prop_table_results_linear <- vector(mode = "list", length = 10)

validation_acc_results_linear <- vector(mode = "list", length = 10)
validation_acc_table_results_linear <- vector(mode = "list", length = 10)
validation_acc_stats_results_linear <- vector(mode = "list", length = 10)
validation_acc_probA_results_linear <- vector(mode = "list", length = 10)
validation_acc_probB_results_linear <- vector(mode = "list", length = 10)
validation_acc_agreement_results_linear <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results_linear <- vector(mode = "list", length = 10)
validation_acc_prop_table_results_linear <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_linear <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj_linear$best.parameters$cost
# train the model with the whole train_set and optimized parameters
  svm.model_linear <- svm(sleep_deprivation~., data = train_set, cost = cost_val, kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
# plotting
# View(train_set)
# plot(svm.model_linear, train_set, w_DMN_fc ~ w_DAN_fc)
# plot(svm.model_polynomial, train_set, w_DMN_fc ~ w_DAN_fc)

# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_linear)
# test
  svm.pred_linear <- predict(svm.model_linear, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_linear <- mean(svm.pred_linear == test_set[,1])
  acc_table_linear <- table(pred = svm.pred_linear, true = test_set[,1])
  acc_stats_linear <- classAgreement(table(pred = svm.pred_linear, true = test_set[,1]))
  acc_probA_linear <- svm.model_linear$probA
# 
  acc_probB_linear <- svm.model_linear$probB
# 

  acc_agreement_linear <- svm.pred_linear == test_set[,1]
  acc_agreement_table_linear <- table(acc_agreement_linear)
  acc_prop_table_linear <- prop.table(table(acc_agreement_linear))
# FALSE  TRUE 
# store test results
  acc_results_linear[[i]] <- accuracy_linear
  acc_table_results_linear[[i]] <- acc_table_linear
  acc_stats_results_linear[[i]] <- acc_stats_linear
  acc_probA_results_linear[[i]] <- acc_probA_linear
  acc_probB_results_linear[[i]] <- acc_probB_linear
  acc_agreement_results_linear[[i]] <- acc_agreement_linear
  acc_agreement_table_results_linear[[i]] <- acc_agreement_table_linear
  acc_prop_table_results_linear[[i]] <- acc_prop_table_linear

# validation
  svm.pred_validation_linear <- predict(svm.model_linear, validation_set[,-1])
  validation_accuracy_linear <- mean(svm.pred_validation_linear == validation_set[,1])
  validation_acc_table_linear <- table(pred = svm.pred_validation_linear, true = validation_set[,1])
  validation_acc_stats_linear <- classAgreement(table(pred = svm.pred_validation_linear, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_linear <- svm.model_linear$probA
#  
  validation_acc_probB_linear <- svm.model_linear$probB
#
  validation_acc_agreement_linear <- svm.pred_validation_linear == validation_set[,1]
  validation_acc_agreement_table_linear <- table(validation_acc_agreement_linear)
  validation_acc_prop_table_linear <- prop.table(table(validation_acc_agreement_linear))
  
  validation_acc_results_linear[[i]] <- validation_accuracy_linear
  validation_acc_table_results_linear[[i]] <- validation_acc_table_linear
  validation_acc_stats_results_linear[[i]] <- validation_acc_stats_linear
  validation_acc_probA_results_linear[[i]] <- validation_acc_probA_linear
  validation_acc_probB_results_linear[[i]] <- validation_acc_probB_linear
  validation_acc_agreement_results_linear[[i]] <- validation_acc_agreement_linear
  validation_acc_agreement_table_results_linear[[i]] <- validation_acc_agreement_table_linear
  validation_acc_prop_table_results_linear[[i]] <- validation_acc_prop_table_linear
}
senior_train_test_young_validate_linear_results <- c(acc_results_linear, acc_table_results_linear, acc_stats_results_linear, acc_probA_results_linear, acc_probB_results_linear, acc_agreement_results_linear, acc_agreement_table_results_linear, acc_prop_table_results_linear, validation_acc_results_linear, validation_acc_table_results_linear, validation_acc_stats_results_linear, validation_acc_probA_results_linear, validation_acc_probB_results_linear, validation_acc_agreement_results_linear, validation_acc_agreement_table_results_linear, validation_acc_prop_table_results_linear)
sink(here("output",'230521_center_mean_senior_train_test_young_validate_linear_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_linear_results)
sink()
```




## young subset, senior validation
### 258 feature (all 258 feature need to be edited)
#### 258 quardramize (all chunks below need to be edited)
```{r 258 old participants sub-dataset}
load(file = here("output", "230511ML_dataset_258_feature_full_balance.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(group_change, participant_label, by = "ID")
TEST <- test_old
```
```{r 258 young participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(group_change, participant_label_validation, by = "ID")

#View(participant)
# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# str(fc_s1$sleep_deprivation)
validation_set <- test_young %>% 
  select(-ID) %>% 
  as.data.frame()
```
##### 258 quardramized linear models 
```{r 258 old participants linear: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear")
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_average_fc_linear_100_repetition_results.csv'))
print(senior_train_test_young_validate_results)
sink()
```
```{r 258 old participants LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE, tunecontrol = tune.control(cross = nrow(train_set)))# .58
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_average_fc_linear_LOO_results.csv'))
print(senior_train_test_young_validate_LOO_results)
sink()
```
##### 258 quardramized RBF models
```{r TBR 258 old participants RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
```{r TBR 258 old participants LOO RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
##### 258 quardramized Polynominal models
```{r TBR 258 old participants Polynominal: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_polynominal_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
```{r TBR_DEFER 258 old participants Polynominal LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_polynominal_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
##### 258 quardramized sigmoid models
```{r TBR 258 old participants sigmoid: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_sigmoid_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
```{r TBR_DEFER 258 old participants Sigmoid LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_sigmoid_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
#### 258 averaged fc non-quardramized (all need to be edited)
```{r 258 old participants sub-dataset}
load(file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(test, participant_label, by = "ID")
TEST <- test_old
```
```{r 258 young participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(test, participant_label_validation, by = "ID")

#View(participant)
# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# str(fc_s1$sleep_deprivation)
validation_set <- test_young %>% 
  select(-ID) %>% 
  as.data.frame()
```
##### 258 non-quardramized linear models 
```{r 258 old participants linear: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear")
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_average_fc_linear_100_repetition_results.csv'))
print(senior_train_test_young_validate_results)
sink()
```
```{r 258 old participants LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE, tunecontrol = tune.control(cross = nrow(train_set)))# .58
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_average_fc_linear_LOO_results.csv'))
print(senior_train_test_young_validate_LOO_results)
sink()
```
##### 258 non-quardramized RBF models
```{r TBR 258 old participants RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
```{r TBR 258 old participants LOO RBF: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_RBF_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```
##### 258 non-quardramized Polynominal models
```{r TBR 258 old participants Polynominal: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_polynominal_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
```{r TBR_DEFER 258 old participants Polynominal LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_polynominal_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
##### 258 non-quardramized sigmoid models
```{r TBR 258 old participants sigmoid: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_sigmoid_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
```{r TBR_DEFER 258 old participants Sigmoid LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230514_senior_train_test_young_validate_sigmoid_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```




#### 258 centering (obselete)
```{r updated old participants sub-dataset}
load(file = here("output", "230511ML_dataset_258_feature_full_balance.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(group_change, participant_label, by = "ID")
TEST <- test_old
```
```{r updated young participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(group_change, participant_label_validation, by = "ID")

#View(participant)
# clarify sleep manipulation condition
# originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# str(fc_s1$sleep_deprivation)
validation_set <- test_young %>% 
  select(-ID) %>% 
  as.data.frame()
```
```{r updated old participants RBF 100 rep: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = FALSE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, scale = FALSE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230511_updated_senior_train_test_young_validate_RBF_results_100_repetition.csv'))
print(senior_train_test_young_validate_RBF_results)
sink()
```

### 29 feature
#### 29 quardramize 
```{r 29 young participants sub-dataset}
load(file = here("output", "230513updated_ML_dataset_group_change_individual_differences.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(group_change, participant_label, by = "ID")
TEST <- test_young
```
```{r 29 old participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(group_change, participant_label_validation, by = "ID")

validation_set <- test_old %>% 
  select(-ID) %>% 
  as.data.frame()
```

##### 29 quardramize RBF models
```{r 29 young participants RBF 100 rep: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
trainsetID <- vector(mode = "list", length = 100)
testsetID <- vector(mode = "list", length = 100)
result_names <- vector(mode = "list", length = 100)
result_decision <- vector(mode = "list", length = 100)
result_probability <- vector(mode = "list", length = 100)
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
#acc_probA_results <- vector(mode = "list", length = 100)
#acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
#validation_acc_probA_results <- vector(mode = "list", length = 100)
#validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = TRUE, probability = TRUE, decision.values = TRUE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, scale = TRUE, probability = TRUE, decision.values = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1], decision.values = TRUE, probability = TRUE)

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
#  acc_probA <- svm.model$probA
# 
#  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
#  acc_probA_results[[i]] <- acc_probA
#  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
#  validation_acc_probA <- svm.model$probA
#  
#  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
#  validation_acc_probA_results[[i]] <- validation_acc_probA
#  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
  
# store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred)$names
  result_decision[[i]] <- attributes(svm.pred)$decision.values
  result_probability[[i]] <- attributes(svm.pred)$probabilities
}
young_train_test_senior_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230612_young_train_test_senior_validate_RBF_results_100_repetition.csv'))
print(young_train_test_senior_validate_RBF_results)
sink()

forced_choice_young_train_test_senior_validate_RBF_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230612_forced_choice_young_train_test_senior_validate_RBF_100_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_RBF_results)
sink()
```
```{r 29 young participants RBF LOO: tune, train, test, validation}
trainsetID <- vector(mode = "list", length = 10)
testsetID <- vector(mode = "list", length = 10)
result_names <- vector(mode = "list", length = 10)
result_decision <- vector(mode = "list", length = 10)
result_probability <- vector(mode = "list", length = 10)
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = TRUE, tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
#  acc_probA <- svm.model$probA
# 
#  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
#  acc_probA_results[[i]] <- acc_probA
#  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
#  validation_acc_probA <- svm.model$probA
#  
#  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
#  validation_acc_probA_results[[i]] <- validation_acc_probA
#  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
  
  # store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred)$names
  result_decision[[i]] <- attributes(svm.pred)$decision.values
  result_probability[[i]] <- attributes(svm.pred)$probabilities
}
young_train_test_senior_validate_RBF_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230612_young_train_test_senior_validate_RBF_LOO_10_repetition_results.csv'))
print(young_train_test_senior_validate_RBF_LOO_results)
sink()

forced_choice_young_train_test_senior_validate_linear_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230612_forced_choice_young_train_test_senior_validate_RBF_LOO_10_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_linear_results)
sink()
```

##### 29 quardramize linear models
```{r 29 young participants linear: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
trainsetID <- vector(mode = "list", length = 100)
testsetID <- vector(mode = "list", length = 100)
result_names <- vector(mode = "list", length = 100)
result_decision <- vector(mode = "list", length = 100)
result_probability <- vector(mode = "list", length = 100)
acc_results_linear <- vector(mode = "list", length = 100)
acc_table_results_linear <- vector(mode = "list", length = 100)
acc_stats_results_linear <- vector(mode = "list", length = 100)
#acc_probA_results_linear <- vector(mode = "list", length = 100)
#acc_probB_results_linear <- vector(mode = "list", length = 100)
acc_agreement_results_linear <- vector(mode = "list", length = 100)
acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
acc_prop_table_results_linear <- vector(mode = "list", length = 100)

validation_acc_results_linear <- vector(mode = "list", length = 100)
validation_acc_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_stats_results_linear <- vector(mode = "list", length = 100)
#validation_acc_probA_results_linear <- vector(mode = "list", length = 100)
#validation_acc_probB_results_linear <- vector(mode = "list", length = 100)
validation_acc_agreement_results_linear <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_prop_table_results_linear <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_linear <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), kernel = "linear", scale = TRUE, probability = TRUE, decision.values = TRUE)
  cost_val <- obj_linear$best.parameters$cost
# train the model with the whole train_set and optimized parameters
  svm.model_linear <- svm(sleep_deprivation~., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE, decision.values = TRUE)
# plotting
# View(train_set)
# plot(svm.model_linear, train_set, w_DMN_fc ~ w_DAN_fc)
# plot(svm.model_polynomial, train_set, w_DMN_fc ~ w_DAN_fc)

# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_linear)
# test
  svm.pred_linear <- predict(svm.model_linear, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_linear <- mean(svm.pred_linear == test_set[,1])
  acc_table_linear <- table(pred = svm.pred_linear, true = test_set[,1])
  acc_stats_linear <- classAgreement(table(pred = svm.pred_linear, true = test_set[,1]))
#  acc_probA_linear <- svm.model_linear$probA
# 
#  acc_probB_linear <- svm.model_linear$probB
# 

  acc_agreement_linear <- svm.pred_linear == test_set[,1]
  acc_agreement_table_linear <- table(acc_agreement_linear)
  acc_prop_table_linear <- prop.table(table(acc_agreement_linear))
# FALSE  TRUE 
# store test results
  acc_results_linear[[i]] <- accuracy_linear
  acc_table_results_linear[[i]] <- acc_table_linear
  acc_stats_results_linear[[i]] <- acc_stats_linear
  acc_probA_results_linear[[i]] <- acc_probA_linear
  acc_probB_results_linear[[i]] <- acc_probB_linear
  acc_agreement_results_linear[[i]] <- acc_agreement_linear
  acc_agreement_table_results_linear[[i]] <- acc_agreement_table_linear
  acc_prop_table_results_linear[[i]] <- acc_prop_table_linear

# validation
  svm.pred_validation_linear <- predict(svm.model_linear, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy_linear <- mean(svm.pred_validation_linear == validation_set[,1])
  validation_acc_table_linear <- table(pred = svm.pred_validation_linear, true = validation_set[,1])
  validation_acc_stats_linear <- classAgreement(table(pred = svm.pred_validation_linear, true = validation_set[,1]))

#  validation_acc_probA_linear <- svm.model_linear$probA
#  
#  validation_acc_probB_linear <- svm.model_linear$probB
#
  validation_acc_agreement_linear <- svm.pred_validation_linear == validation_set[,1]
  validation_acc_agreement_table_linear <- table(validation_acc_agreement_linear)
  validation_acc_prop_table_linear <- prop.table(table(validation_acc_agreement_linear))
  
  validation_acc_results_linear[[i]] <- validation_accuracy_linear
  validation_acc_table_results_linear[[i]] <- validation_acc_table_linear
  validation_acc_stats_results_linear[[i]] <- validation_acc_stats_linear
#  validation_acc_probA_results_linear[[i]] <- validation_acc_probA_linear
#  validation_acc_probB_results_linear[[i]] <- validation_acc_probB_linear
  validation_acc_agreement_results_linear[[i]] <- validation_acc_agreement_linear
  validation_acc_agreement_table_results_linear[[i]] <- validation_acc_agreement_table_linear
  validation_acc_prop_table_results_linear[[i]] <- validation_acc_prop_table_linear

  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred_linear)$names
  result_decision[[i]] <- attributes(svm.pred_linear)$decision.values
  result_probability[[i]] <- attributes(svm.pred_linear)$probabilities
  }
young_train_test_senior_validate_linear_results <- c(acc_results_linear, acc_table_results_linear, acc_stats_results_linear, acc_agreement_results_linear, acc_agreement_table_results_linear, acc_prop_table_results_linear, validation_acc_results_linear, validation_acc_table_results_linear, validation_acc_stats_results_linear, validation_acc_agreement_results_linear, validation_acc_agreement_table_results_linear, validation_acc_prop_table_results_linear)
sink(here("output",'230612_young_train_test_senior_validate_linear_100_repetition_results.csv'))
print(young_train_test_senior_validate_linear_results)
sink()

forced_choice_young_train_test_senior_validate_linear_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230612_forced_choice_young_train_test_senior_validate_linear_100_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_linear_results)
sink()
```
```{r 29 young participants linear LOO: tune, train, test, validation}
trainsetID <- vector(mode = "list", length = 100)
testsetID <- vector(mode = "list", length = 100)
result_names <- vector(mode = "list", length = 100)
result_decision <- vector(mode = "list", length = 100)
result_probability <- vector(mode = "list", length = 100)
acc_results_linear <- vector(mode = "list", length = 100)
acc_table_results_linear <- vector(mode = "list", length = 100)
acc_stats_results_linear <- vector(mode = "list", length = 100)
#acc_probA_results_linear <- vector(mode = "list", length = 10)
#acc_probB_results_linear <- vector(mode = "list", length = 100)
acc_agreement_results_linear <- vector(mode = "list", length = 100)
acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
acc_prop_table_results_linear <- vector(mode = "list", length = 100)

validation_acc_results_linear <- vector(mode = "list", length = 100)
validation_acc_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_stats_results_linear <- vector(mode = "list", length = 100)
#validation_acc_probA_results_linear <- vector(mode = "list", length = 10)
#validation_acc_probB_results_linear <- vector(mode = "list", length = 10)
validation_acc_agreement_results_linear <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_prop_table_results_linear <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_linear <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
  cost_val <- obj_linear$best.parameters$cost
# train the model with the whole train_set and optimized parameters
  svm.model_linear <- svm(sleep_deprivation~., data = train_set, cost = cost_val, kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
# plotting
# View(train_set)
# plot(svm.model_linear, train_set, w_DMN_fc ~ w_DAN_fc)
# plot(svm.model_polynomial, train_set, w_DMN_fc ~ w_DAN_fc)

# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_linear)
# test
  svm.pred_linear <- predict(svm.model_linear, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_linear <- mean(svm.pred_linear == test_set[,1])
  acc_table_linear <- table(pred = svm.pred_linear, true = test_set[,1])
  acc_stats_linear <- classAgreement(table(pred = svm.pred_linear, true = test_set[,1]))
#  acc_probA_linear <- svm.model_linear$probA
# 
#  acc_probB_linear <- svm.model_linear$probB
# 

  acc_agreement_linear <- svm.pred_linear == test_set[,1]
  acc_agreement_table_linear <- table(acc_agreement_linear)
  acc_prop_table_linear <- prop.table(table(acc_agreement_linear))
# FALSE  TRUE 
# store test results
  acc_results_linear[[i]] <- accuracy_linear
  acc_table_results_linear[[i]] <- acc_table_linear
  acc_stats_results_linear[[i]] <- acc_stats_linear
  #acc_probA_results_linear[[i]] <- acc_probA_linear
  #acc_probB_results_linear[[i]] <- acc_probB_linear
  acc_agreement_results_linear[[i]] <- acc_agreement_linear
  acc_agreement_table_results_linear[[i]] <- acc_agreement_table_linear
  acc_prop_table_results_linear[[i]] <- acc_prop_table_linear

# validation
  svm.pred_validation_linear <- predict(svm.model_linear, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy_linear <- mean(svm.pred_validation_linear == validation_set[,1])
  validation_acc_table_linear <- table(pred = svm.pred_validation_linear, true = validation_set[,1])
  validation_acc_stats_linear <- classAgreement(table(pred = svm.pred_validation_linear, true = validation_set[,1]))
#  validation_acc_probA_linear <- svm.pred_validation_linear$probA
#  
#  validation_acc_probB_linear <- svm.pred_validation_linear$probB
#
  validation_acc_agreement_linear <- svm.pred_validation_linear == validation_set[,1]
  validation_acc_agreement_table_linear <- table(validation_acc_agreement_linear)
  validation_acc_prop_table_linear <- prop.table(table(validation_acc_agreement_linear))
  
  validation_acc_results_linear[[i]] <- validation_accuracy_linear
  validation_acc_table_results_linear[[i]] <- validation_acc_table_linear
  validation_acc_stats_results_linear[[i]] <- validation_acc_stats_linear
  #validation_acc_probA_results_linear[[i]] <- validation_acc_probA_linear
  #validation_acc_probB_results_linear[[i]] <- validation_acc_probB_linear
  validation_acc_agreement_results_linear[[i]] <- validation_acc_agreement_linear
  validation_acc_agreement_table_results_linear[[i]] <- validation_acc_agreement_table_linear
  validation_acc_prop_table_results_linear[[i]] <- validation_acc_prop_table_linear
  
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred_linear)$names
  result_decision[[i]] <- attributes(svm.pred_linear)$decision.values
  result_probability[[i]] <- attributes(svm.pred_linear)$probabilities
}
young_train_test_senior_validate_linear_results <- c(acc_results_linear, acc_table_results_linear, acc_stats_results_linear, acc_agreement_results_linear, acc_agreement_table_results_linear, acc_prop_table_results_linear, validation_acc_results_linear, validation_acc_table_results_linear, validation_acc_stats_results_linear, validation_acc_agreement_results_linear, validation_acc_agreement_table_results_linear, validation_acc_prop_table_results_linear)
sink(here("output",'230612_young_train_test_senior_validate_linear_LOO_100_repetition_results.csv'))
print(young_train_test_senior_validate_linear_results)
sink()

forced_choice_young_train_test_senior_validate_linear_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230612_forced_choice_young_train_test_senior_validate_linear_LOO_100_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_linear_results)
sink()
```

##### 29 quardramize Polynominal models TAKING TOO MUCH TIME TO RUN AND WARNING ABOUT REACHING MAX NUM OF ITERATIONS
```{r TBR 29 old participants polynomial: tune, train, test, validation}
acc_results_polynomial <- vector(mode = "list", length = 100)
acc_table_results_polynomial <- vector(mode = "list", length = 100)
acc_stats_results_polynomial <- vector(mode = "list", length = 100)
acc_probA_results_polynomial <- vector(mode = "list", length = 100)
acc_probB_results_polynomial <- vector(mode = "list", length = 100)
acc_agreement_results_polynomial <- vector(mode = "list", length = 100)
acc_agreement_table_results_polynomial <- vector(mode = "list", length = 100)
acc_prop_table_results_polynomial <- vector(mode = "list", length = 100)

validation_acc_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_table_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_stats_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_probA_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_probB_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_agreement_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results_polynomial <- vector(mode = "list", length = 100)
validation_acc_prop_table_results_polynomial <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_polynomial <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial")
  cost_val <- obj_polynomial$best.parameters$cost
  gamma_val <- obj_polynomial$best.parameters$gamma
  coef0_val <- obj_polynomial$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model_polynomial <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial")
# grep("sleep_deprivation", colnames(train_set))
#summary(svm.model_polynomial)
# test
  svm.pred_polynomial <- predict(svm.model_polynomial, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_polynomial <- mean(svm.pred_polynomial == test_set[,1])
  acc_table_polynomial <- table(pred = svm.pred_polynomial, true = test_set[,1])
  acc_stats_polynomial <- classAgreement(table(pred = svm.pred_polynomial, true = test_set[,1]))
  acc_probA_polynomial <- svm.model_polynomial$probA
# 
  acc_probB_polynomial <- svm.model_polynomial$probB
# 

  acc_agreement_polynomial <- svm.pred_polynomial == test_set[,1]
  acc_agreement_table_polynomial <- table(acc_agreement_polynomial)
  acc_prop_table_polynomial <- prop.table(table(acc_agreement_polynomial))
# FALSE  TRUE 
# store test results
  acc_results_polynomial[[i]] <- accuracy_polynomial
  acc_table_results_polynomial[[i]] <- acc_table_polynomial
  acc_stats_results_polynomial[[i]] <- acc_stats_polynomial
  acc_probA_results_polynomial[[i]] <- acc_probA_polynomial
  acc_probB_results_polynomial[[i]] <- acc_probB_polynomial
  acc_agreement_results_polynomial[[i]] <- acc_agreement_polynomial
  acc_agreement_table_results_polynomial[[i]] <- acc_agreement_table_polynomial
  acc_prop_table_results_polynomial[[i]] <- acc_prop_table_polynomial

# validation
  svm.pred_validation_polynomial <- predict(svm.model_polynomial, validation_set[,-1])
  validation_accuracy_polynomial <- mean(svm.pred_validation_polynomial == validation_set[,1])
  validation_acc_table_polynomial <- table(pred = svm.pred_validation_polynomial, true = validation_set[,1])
  validation_acc_stats_polynomial <- classAgreement(table(pred = svm.pred_validation_polynomial, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_polynomial <- svm.model_polynomial$probA
#  
  validation_acc_probB_polynomial <- svm.model_polynomial$probB
#
  validation_acc_agreement_polynomial <- svm.pred_validation_polynomial == validation_set[,1]
  validation_acc_agreement_table_polynomial <- table(validation_acc_agreement_polynomial)
  validation_acc_prop_table_polynomial <- prop.table(table(validation_acc_agreement_polynomial))
  
  validation_acc_results_polynomial[[i]] <- validation_accuracy_polynomial
  validation_acc_table_results_polynomial[[i]] <- validation_acc_table_polynomial
  validation_acc_stats_results_polynomial[[i]] <- validation_acc_stats_polynomial
  validation_acc_probA_results_polynomial[[i]] <- validation_acc_probA_polynomial
  validation_acc_probB_results_polynomial[[i]] <- validation_acc_probB_polynomial
  validation_acc_agreement_results_polynomial[[i]] <- validation_acc_agreement_polynomial
  validation_acc_agreement_table_results_polynomial[[i]] <- validation_acc_agreement_table_polynomial
  validation_acc_prop_table_results_polynomial[[i]] <- validation_acc_prop_table_polynomial
}
senior_train_test_young_validate_polynomial_results <- c(acc_results_polynomial, acc_table_results_polynomial, acc_stats_results_polynomial, acc_probA_results_polynomial, acc_probB_results_polynomial, acc_agreement_results_polynomial, acc_agreement_table_results_polynomial, acc_prop_table_results_polynomial, validation_acc_results_polynomial, validation_acc_table_results_polynomial, validation_acc_stats_results_polynomial, validation_acc_probA_results_polynomial, validation_acc_probB_results_polynomial, validation_acc_agreement_results_polynomial, validation_acc_agreement_table_results_polynomial, validation_acc_prop_table_results_polynomial)
sink(here("output",'230515_senior_train_test_young_validate_polynominal_100_repetition_results.csv'))
print(senior_train_test_young_validate_polynomial_results)
sink()
```
```{r TBR 29 old participants polynomial LOO: tune, train, test, validation}
acc_results_polynomial <- vector(mode = "list", length = 10)
acc_table_results_polynomial <- vector(mode = "list", length = 10)
acc_stats_results_polynomial <- vector(mode = "list", length = 10)
acc_probA_results_polynomial <- vector(mode = "list", length = 10)
acc_probB_results_polynomial <- vector(mode = "list", length = 10)
acc_agreement_results_polynomial <- vector(mode = "list", length = 10)
acc_agreement_table_results_polynomial <- vector(mode = "list", length = 10)
acc_prop_table_results_polynomial <- vector(mode = "list", length = 10)

validation_acc_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_table_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_stats_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_probA_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_probB_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_agreement_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results_polynomial <- vector(mode = "list", length = 10)
validation_acc_prop_table_results_polynomial <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj_polynomial <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj_polynomial$best.parameters$cost
  gamma_val <- obj_polynomial$best.parameters$gamma
  coef0_val <- obj_polynomial$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model_polynomial <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_polynomial)
# test
  svm.pred_polynomial <- predict(svm.model_polynomial, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_polynomial <- mean(svm.pred_polynomial == test_set[,1])
  acc_table_polynomial <- table(pred = svm.pred_polynomial, true = test_set[,1])
  acc_stats_polynomial <- classAgreement(table(pred = svm.pred_polynomial, true = test_set[,1]))
  acc_probA_polynomial <- svm.model_polynomial$probA
# 
  acc_probB_polynomial <- svm.model_polynomial$probB
# 

  acc_agreement_polynomial <- svm.pred_polynomial == test_set[,1]
  acc_agreement_table_polynomial <- table(acc_agreement_polynomial)
  acc_prop_table_polynomial <- prop.table(table(acc_agreement_polynomial))
# FALSE  TRUE 
# store test results
  acc_results_polynomial[[i]] <- accuracy_polynomial
  acc_table_results_polynomial[[i]] <- acc_table_polynomial
  acc_stats_results_polynomial[[i]] <- acc_stats_polynomial
  acc_probA_results_polynomial[[i]] <- acc_probA_polynomial
  acc_probB_results_polynomial[[i]] <- acc_probB_polynomial
  acc_agreement_results_polynomial[[i]] <- acc_agreement_polynomial
  acc_agreement_table_results_polynomial[[i]] <- acc_agreement_table_polynomial
  acc_prop_table_results_polynomial[[i]] <- acc_prop_table_polynomial

# validation
  svm.pred_validation_polynomial <- predict(svm.model_polynomial, validation_set[,-1])
  validation_accuracy_polynomial <- mean(svm.pred_validation_polynomial == validation_set[,1])
  validation_acc_table_polynomial <- table(pred = svm.pred_validation_polynomial, true = validation_set[,1])
  validation_acc_stats_polynomial <- classAgreement(table(pred = svm.pred_validation_polynomial, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_polynomial <- svm.model_polynomial$probA
#  
  validation_acc_probB_polynomial <- svm.model_polynomial$probB
#
  validation_acc_agreement_polynomial <- svm.pred_validation_polynomial == validation_set[,1]
  validation_acc_agreement_table_polynomial <- table(validation_acc_agreement_polynomial)
  validation_acc_prop_table_polynomial <- prop.table(table(validation_acc_agreement_polynomial))
  
  validation_acc_results_polynomial[[i]] <- validation_accuracy_polynomial
  validation_acc_table_results_polynomial[[i]] <- validation_acc_table_polynomial
  validation_acc_stats_results_polynomial[[i]] <- validation_acc_stats_polynomial
  validation_acc_probA_results_polynomial[[i]] <- validation_acc_probA_polynomial
  validation_acc_probB_results_polynomial[[i]] <- validation_acc_probB_polynomial
  validation_acc_agreement_results_polynomial[[i]] <- validation_acc_agreement_polynomial
  validation_acc_agreement_table_results_polynomial[[i]] <- validation_acc_agreement_table_polynomial
  validation_acc_prop_table_results_polynomial[[i]] <- validation_acc_prop_table_polynomial
}
senior_train_test_young_validate_polynominal_LOO_10_repetition_results <- c(acc_results_polynomial, acc_table_results_polynomial, acc_stats_results_polynomial, acc_probA_results_polynomial, acc_probB_results_polynomial, acc_agreement_results_polynomial, acc_agreement_table_results_polynomial, acc_prop_table_results_polynomial, validation_acc_results_polynomial, validation_acc_table_results_polynomial, validation_acc_stats_results_polynomial, validation_acc_probA_results_polynomial, validation_acc_probB_results_polynomial, validation_acc_agreement_results_polynomial, validation_acc_agreement_table_results_polynomial, validation_acc_prop_table_results_polynomial)
sink(here("output",'230515_senior_train_test_young_validate_polynominal_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_LOO_10_repetition_results)
sink()
```
##### 29 quardramize sigmoid models TAKING TOO MUCH TIME TO RUN AND WARNING ABOUT REACHING MAX NUM OF ITERATIONS
```{r TBR 29 old participants sigmoid: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
acc_results_sigmoid <- vector(mode = "list", length = 100)
acc_table_results_sigmoid <- vector(mode = "list", length = 100)
acc_stats_results_sigmoid <- vector(mode = "list", length = 100)
acc_probA_results_sigmoid <- vector(mode = "list", length = 100)
acc_probB_results_sigmoid <- vector(mode = "list", length = 100)
acc_agreement_results_sigmoid <- vector(mode = "list", length = 100)
acc_agreement_table_results_sigmoid <- vector(mode = "list", length = 100)
acc_prop_table_results_sigmoid <- vector(mode = "list", length = 100)

validation_acc_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_table_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_stats_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_probA_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_probB_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_agreement_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results_sigmoid <- vector(mode = "list", length = 100)
validation_acc_prop_table_results_sigmoid <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_sigmoid <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid")
  cost_val <- obj_sigmoid$best.parameters$cost
  gamma_val <- obj_sigmoid$best.parameters$gamma
  coef0_val <- obj_sigmoid$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model_sigmoid <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid")
# grep("sleep_deprivation", colnames(train_set))
#summary(svm.model_sigmoid)
# test
  svm.pred_sigmoid <- predict(svm.model_sigmoid, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_sigmoid <- mean(svm.pred_sigmoid == test_set[,1])
  acc_table_sigmoid <- table(pred = svm.pred_sigmoid, true = test_set[,1])
  acc_stats_sigmoid <- classAgreement(table(pred = svm.pred_sigmoid, true = test_set[,1]))
  acc_probA_sigmoid <- svm.model_sigmoid$probA
# 
  acc_probB_sigmoid <- svm.model_sigmoid$probB
# 

  acc_agreement_sigmoid <- svm.pred_sigmoid == test_set[,1]
  acc_agreement_table_sigmoid <- table(acc_agreement_sigmoid)
  acc_prop_table_sigmoid <- prop.table(table(acc_agreement_sigmoid))
# FALSE  TRUE 
# store test results
  acc_results_sigmoid[[i]] <- accuracy_sigmoid
  acc_table_results_sigmoid[[i]] <- acc_table_sigmoid
  acc_stats_results_sigmoid[[i]] <- acc_stats_sigmoid
  acc_probA_results_sigmoid[[i]] <- acc_probA_sigmoid
  acc_probB_results_sigmoid[[i]] <- acc_probB_sigmoid
  acc_agreement_results_sigmoid[[i]] <- acc_agreement_sigmoid
  acc_agreement_table_results_sigmoid[[i]] <- acc_agreement_table_sigmoid
  acc_prop_table_results_sigmoid[[i]] <- acc_prop_table_sigmoid

# validation
  svm.pred_validation_sigmoid <- predict(svm.model_sigmoid, validation_set[,-1])
  validation_accuracy_sigmoid <- mean(svm.pred_validation_sigmoid == validation_set[,1])
  validation_acc_table_sigmoid <- table(pred = svm.pred_validation_sigmoid, true = validation_set[,1])
  validation_acc_stats_sigmoid <- classAgreement(table(pred = svm.pred_validation_sigmoid, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_sigmoid <- svm.model_sigmoid$probA
#  
  validation_acc_probB_sigmoid <- svm.model_sigmoid$probB
#
  validation_acc_agreement_sigmoid <- svm.pred_validation_sigmoid == validation_set[,1]
  validation_acc_agreement_table_sigmoid <- table(validation_acc_agreement_sigmoid)
  validation_acc_prop_table_sigmoid <- prop.table(table(validation_acc_agreement_sigmoid))
  
  validation_acc_results_sigmoid[[i]] <- validation_accuracy_sigmoid
  validation_acc_table_results_sigmoid[[i]] <- validation_acc_table_sigmoid
  validation_acc_stats_results_sigmoid[[i]] <- validation_acc_stats_sigmoid
  validation_acc_probA_results_sigmoid[[i]] <- validation_acc_probA_sigmoid
  validation_acc_probB_results_sigmoid[[i]] <- validation_acc_probB_sigmoid
  validation_acc_agreement_results_sigmoid[[i]] <- validation_acc_agreement_sigmoid
  validation_acc_agreement_table_results_sigmoid[[i]] <- validation_acc_agreement_table_sigmoid
  validation_acc_prop_table_results_sigmoid[[i]] <- validation_acc_prop_table_sigmoid
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results_sigmoid, acc_table_results_sigmoid, acc_stats_results_sigmoid, acc_probA_results_sigmoid, acc_probB_results_sigmoid, acc_agreement_results_sigmoid, acc_agreement_table_results_sigmoid, acc_prop_table_results_sigmoid, validation_acc_results_sigmoid, validation_acc_table_results_sigmoid, validation_acc_stats_results_sigmoid, validation_acc_probA_results_sigmoid, validation_acc_probB_results_sigmoid, validation_acc_agreement_results_sigmoid, validation_acc_agreement_table_results_sigmoid, validation_acc_prop_table_results_sigmoid)
sink(here("output",'230513_updated_senior_train_test_young_validate_sigmoid_100_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
```{r TBR 29 updated old participants sigmoid LOO: tune, train, test, validation}
acc_results_sigmoid <- vector(mode = "list", length = 10)
acc_table_results_sigmoid <- vector(mode = "list", length = 10)
acc_stats_results_sigmoid <- vector(mode = "list", length = 10)
acc_probA_results_sigmoid <- vector(mode = "list", length = 10)
acc_probB_results_sigmoid <- vector(mode = "list", length = 10)
acc_agreement_results_sigmoid <- vector(mode = "list", length = 10)
acc_agreement_table_results_sigmoid <- vector(mode = "list", length = 10)
acc_prop_table_results_sigmoid <- vector(mode = "list", length = 10)

validation_acc_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_table_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_stats_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_probA_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_probB_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_agreement_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results_sigmoid <- vector(mode = "list", length = 10)
validation_acc_prop_table_results_sigmoid <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj_sigmoid <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj_sigmoid$best.parameters$cost
  gamma_val <- obj_sigmoid$best.parameters$gamma
  coef0_val <- obj_sigmoid$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model_sigmoid <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))
# summary(svm.model_sigmoid)
# test
  svm.pred_sigmoid <- predict(svm.model_sigmoid, test_set[,-1])

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_sigmoid <- mean(svm.pred_sigmoid == test_set[,1])
  acc_table_sigmoid <- table(pred = svm.pred_sigmoid, true = test_set[,1])
  acc_stats_sigmoid <- classAgreement(table(pred = svm.pred_sigmoid, true = test_set[,1]))
  acc_probA_sigmoid <- svm.model_sigmoid$probA
# 
  acc_probB_sigmoid <- svm.model_sigmoid$probB
# 

  acc_agreement_sigmoid <- svm.pred_sigmoid == test_set[,1]
  acc_agreement_table_sigmoid <- table(acc_agreement_sigmoid)
  acc_prop_table_sigmoid <- prop.table(table(acc_agreement_sigmoid))
# FALSE  TRUE 
# store test results
  acc_results_sigmoid[[i]] <- accuracy_sigmoid
  acc_table_results_sigmoid[[i]] <- acc_table_sigmoid
  acc_stats_results_sigmoid[[i]] <- acc_stats_sigmoid
  acc_probA_results_sigmoid[[i]] <- acc_probA_sigmoid
  acc_probB_results_sigmoid[[i]] <- acc_probB_sigmoid
  acc_agreement_results_sigmoid[[i]] <- acc_agreement_sigmoid
  acc_agreement_table_results_sigmoid[[i]] <- acc_agreement_table_sigmoid
  acc_prop_table_results_sigmoid[[i]] <- acc_prop_table_sigmoid

# validation
  svm.pred_validation_sigmoid <- predict(svm.model_sigmoid, validation_set[,-1])
  validation_accuracy_sigmoid <- mean(svm.pred_validation_sigmoid == validation_set[,1])
  validation_acc_table_sigmoid <- table(pred = svm.pred_validation_sigmoid, true = validation_set[,1])
  validation_acc_stats_sigmoid <- classAgreement(table(pred = svm.pred_validation_sigmoid, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA_sigmoid <- svm.model_sigmoid$probA
#  
  validation_acc_probB_sigmoid <- svm.model_sigmoid$probB
#
  validation_acc_agreement_sigmoid <- svm.pred_validation_sigmoid == validation_set[,1]
  validation_acc_agreement_table_sigmoid <- table(validation_acc_agreement_sigmoid)
  validation_acc_prop_table_sigmoid <- prop.table(table(validation_acc_agreement_sigmoid))
  
  validation_acc_results_sigmoid[[i]] <- validation_accuracy_sigmoid
  validation_acc_table_results_sigmoid[[i]] <- validation_acc_table_sigmoid
  validation_acc_stats_results_sigmoid[[i]] <- validation_acc_stats_sigmoid
  validation_acc_probA_results_sigmoid[[i]] <- validation_acc_probA_sigmoid
  validation_acc_probB_results_sigmoid[[i]] <- validation_acc_probB_sigmoid
  validation_acc_agreement_results_sigmoid[[i]] <- validation_acc_agreement_sigmoid
  validation_acc_agreement_table_results_sigmoid[[i]] <- validation_acc_agreement_table_sigmoid
  validation_acc_prop_table_results_sigmoid[[i]] <- validation_acc_prop_table_sigmoid
}
senior_train_test_young_validate_sigmoid_LOO_10_repetition_results <- c(acc_results_sigmoid, acc_table_results_sigmoid, acc_stats_results_sigmoid, acc_probA_results_sigmoid, acc_probB_results_sigmoid, acc_agreement_results_sigmoid, acc_agreement_table_results_sigmoid, acc_prop_table_results_sigmoid, validation_acc_results_sigmoid, validation_acc_table_results_sigmoid, validation_acc_stats_results_sigmoid, validation_acc_probA_results_sigmoid, validation_acc_probB_results_sigmoid, validation_acc_agreement_results_sigmoid, validation_acc_agreement_table_results_sigmoid, validation_acc_prop_table_results_sigmoid)
sink(here("output",'230515_senior_train_test_young_validate_sigmoid_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_LOO_10_repetition_results)
sink()
```

#### 29 averaged fc non-quardramized
```{r 29 young participants sub-dataset}
load(file = here("output", "230512ML_dataset_29_feature_full_balance.Rdata"))
participant_label <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(test, participant_label, by = "ID")
TEST <- test_young
```
```{r 29 old participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(test, participant_label_validation, by = "ID")

validation_set <- test_old %>% 
  select(-ID) %>% 
  as.data.frame()
```

##### 29 non-quardramized linear models 
```{r 29 young participants linear: tune, train, test, validation}
trainsetID <- vector(mode = "list", length = 100)
testsetID <- vector(mode = "list", length = 100)
result_names <- vector(mode = "list", length = 100)
result_decision <- vector(mode = "list", length = 100)
result_probability <- vector(mode = "list", length = 100)
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
#acc_probA_results <- vector(mode = "list", length = 100)
#acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
#validation_acc_probA_results <- vector(mode = "list", length = 100)
#validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", probability = TRUE, decision.values = TRUE)
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE, decision.values = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
#  acc_probA <- svm.model$probA
# 
#  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
#  acc_probA_results[[i]] <- acc_probA
#  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
#  validation_acc_probA <- svm.model$probA
#  
#  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
#  validation_acc_probA_results[[i]] <- validation_acc_probA
#  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
  
  # store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred)$names
  result_decision[[i]] <- attributes(svm.pred)$decision.values
  result_probability[[i]] <- attributes(svm.pred)$probabilities
}
young_train_test_senior_validate_results <- c(acc_results, acc_table_results, acc_stats_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230613_young_train_test_senior_validate_average_fc_linear_100_repetition_results.csv'))
print(young_train_test_senior_validate_results)
sink()

forced_choice_young_train_test_senior_validate_linear_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230613_forced_choice_young_train_test_senior_validateaverage_fc_linear_100_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_linear_results)
sink()
```
```{r 29 young participants LOO: tune, train, test, validation}
trainsetID <- vector(mode = "list", length = 10)
testsetID <- vector(mode = "list", length = 10)
result_names <- vector(mode = "list", length = 10)
result_decision <- vector(mode = "list", length = 10)
result_probability <- vector(mode = "list", length = 10)
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
#acc_probA_results <- vector(mode = "list", length = 10)
#acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
#validation_acc_probA_results <- vector(mode = "list", length = 10)
#validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
  cost_val <- obj$best.parameters$cost
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = cost_val, kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
# grep("sleep_deprivation", colnames(train_set))
  
# test
  svm.pred <- predict(svm.model, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
#  acc_probA <- svm.model$probA
# 
#  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
#  acc_probA_results[[i]] <- acc_probA
#  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
#  validation_acc_probA <- svm.model$probA
#  
#  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
#  validation_acc_probA_results[[i]] <- validation_acc_probA
#  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
  
  # store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred)$names
  result_decision[[i]] <- attributes(svm.pred)$decision.values
  result_probability[[i]] <- attributes(svm.pred)$probabilities
}
young_train_test_senior_validate_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230613_young_train_test_senior_validate_average_fc_linear_LOO_results.csv'))
print(young_train_test_senior_validate_LOO_results)
sink()

forced_choice_young_train_test_senior_validate_linear_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230612_forced_choice_young_train_test_senior_validate_average_fc_linear_LOO_results.csv'))
print(forced_choice_young_train_test_senior_validate_linear_results)
sink()
```

##### 29 non-quardramized RBF models
```{r young participants RBF: tune, train, test, validation}
trainsetID <- vector(mode = "list", length = 100)
testsetID <- vector(mode = "list", length = 100)
result_names <- vector(mode = "list", length = 100)
result_decision <- vector(mode = "list", length = 100)
result_probability <- vector(mode = "list", length = 100)
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
#acc_probA_results <- vector(mode = "list", length = 100)
#acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
#validation_acc_probA_results <- vector(mode = "list", length = 100)
#validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), probability = TRUE, decision.values = TRUE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, probability = TRUE, decision.values = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
#  acc_probA <- svm.model$probA
# 
#  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
#  acc_probA_results[[i]] <- acc_probA
#  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))

#  validation_acc_probA <- svm.model$probA
#  
#  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
#  validation_acc_probA_results[[i]] <- validation_acc_probA
#  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
  
  # store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred)$names
  result_decision[[i]] <- attributes(svm.pred)$decision.values
  result_probability[[i]] <- attributes(svm.pred)$probabilities
}
young_train_test_senior_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230613_young_train_test_senior_validate_RBF_average_fc_100_repetition_results.csv'))
print(young_train_test_senior_validate_RBF_results)
sink()

forced_choice_young_train_test_senior_validate_RBF_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230612_forced_choice_young_train_test_senior_validate_RBF_average_fc_100_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_RBF_results)
sink()
```
```{r young participants LOO RBF: tune, train, test, validation}
trainsetID <- vector(mode = "list", length = 10)
testsetID <- vector(mode = "list", length = 10)
result_names <- vector(mode = "list", length = 10)
result_decision <- vector(mode = "list", length = 10)
result_probability <- vector(mode = "list", length = 10)
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
#acc_probA_results <- vector(mode = "list", length = 10)
#acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
#validation_acc_probA_results <- vector(mode = "list", length = 10)
#validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
#  acc_probA <- svm.model$probA
# 
#  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
#  acc_probA_results[[i]] <- acc_probA
#  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
#  validation_acc_probA <- svm.model$probA
#  
#  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
#  validation_acc_probA_results[[i]] <- validation_acc_probA
#  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
  
  # store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred)$names
  result_decision[[i]] <- attributes(svm.pred)$decision.values
  result_probability[[i]] <- attributes(svm.pred)$probabilities  
}
young_train_test_senior_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230613_young_train_test_senior_validate_RBF_average_fc_LOO_10_repetition_results.csv'))
print(young_train_test_senior_validate_RBF_results)
sink()

forced_choice_young_train_test_senior_validate_RBF_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230613_forced_choice_young_train_test_senior_validate_RBF_average_fc_LOO_10_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_RBF_results)
sink()
```

##### 29 non-quardramized Polynominal models TAKING TOO MUCH TIME TO RUN AND WARNING ABOUT REACHING MAX NUM OF ITERATIONS
```{r TBR old participants Polynominal: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_polynominal_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
```{r TBR_DEFER old participants Polynominal LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "polynomial", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_polynominal_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_polynominal_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_polynominal_results)
sink()
```
##### 29 non-quardramized sigmoid models TAKING TOO MUCH TIME TO RUN AND WARNING ABOUT REACHING MAX NUM OF ITERATIONS
```{r TBR old participants sigmoid: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
acc_probA_results <- vector(mode = "list", length = 100)
acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
validation_acc_probA_results <- vector(mode = "list", length = 100)
validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid")
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid")
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_sigmoid_average_fc_100_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```
```{r TBR_DEFER old participants Sigmoid LOO: tune, train, test, validation}
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]

  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)

  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)
  
  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
  coef0_val <- obj$best.parameters$coef0
# summary(obj)
# plot(obj)

    svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, coef0 = coef0_val, kernel = "sigmoid", tunecontrol = tune.control(cross = nrow(train_set)))
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1])

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
  acc_probA <- svm.model$probA
# 
  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
  acc_probA_results[[i]] <- acc_probA
  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1])
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
# # 0.8488372
  validation_acc_probA <- svm.model$probA
#  
  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
  validation_acc_probA_results[[i]] <- validation_acc_probA
  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
}
senior_train_test_young_validate_sigmoid_results <- c(acc_results, acc_table_results, acc_stats_results, acc_probA_results, acc_probB_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_probA_results, validation_acc_probB_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230515_senior_train_test_young_validate_sigmoid_average_fc_LOO_10_repetition_results.csv'))
print(senior_train_test_young_validate_sigmoid_results)
sink()
```


#### 29 centering with mean
```{r 29 young participants sub-dataset}
load(file = here("output", "230520ML_dataset_29_feature_full_balance_center_mean.Rdata"))

participant_label <- participant %>% 
  filter(age_group == "Young") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_young <- inner_join(test_full_scaled, participant_label, by = "ID")
TEST <- test_young
```
```{r 29 old participants validation dataset}
participant_label_validation <- participant %>% 
  filter(age_group == "Old") %>% 
  select(conn_id) %>% 
  rename(ID = conn_id)
test_old <- inner_join(test_full_scaled, participant_label_validation, by = "ID")

validation_set <- test_old %>% 
  select(-ID) %>% 
  as.data.frame()
```

##### 29 centering RBF models
```{r 29 young participants RBF 100 rep: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
trainsetID <- vector(mode = "list", length = 100)
testsetID <- vector(mode = "list", length = 100)
result_names <- vector(mode = "list", length = 100)
result_decision <- vector(mode = "list", length = 100)
result_probability <- vector(mode = "list", length = 100)
acc_results <- vector(mode = "list", length = 100)
acc_table_results <- vector(mode = "list", length = 100)
acc_stats_results <- vector(mode = "list", length = 100)
#acc_probA_results <- vector(mode = "list", length = 100)
#acc_probB_results <- vector(mode = "list", length = 100)
acc_agreement_results <- vector(mode = "list", length = 100)
acc_agreement_table_results <- vector(mode = "list", length = 100)
acc_prop_table_results <- vector(mode = "list", length = 100)

validation_acc_results <- vector(mode = "list", length = 100)
validation_acc_table_results <- vector(mode = "list", length = 100)
validation_acc_stats_results <- vector(mode = "list", length = 100)
#validation_acc_probA_results <- vector(mode = "list", length = 100)
#validation_acc_probB_results <- vector(mode = "list", length = 100)
validation_acc_agreement_results <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results <- vector(mode = "list", length = 100)
validation_acc_prop_table_results <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), scale = TRUE, probability = TRUE, decision.values = TRUE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, scale = TRUE, probability = TRUE, decision.values = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
#  acc_probA <- svm.model$probA
# 
#  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
#  acc_probA_results[[i]] <- acc_probA
#  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
#  validation_acc_probA <- svm.model$probA
#  
#  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
#  validation_acc_probA_results[[i]] <- validation_acc_probA
#  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
  
  # store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred)$names
  result_decision[[i]] <- attributes(svm.pred)$decision.values
  result_probability[[i]] <- attributes(svm.pred)$probabilities
}
young_train_test_senior_validate_RBF_results <- c(acc_results, acc_table_results, acc_stats_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230613_center_mean_young_train_test_senior_validate_RBF_results_100_repetition.csv'))
print(young_train_test_senior_validate_RBF_results)
sink()

forced_choice_young_train_test_senior_validate_RBF_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230613_forced_choice_young_train_test_senior_validate_RBF_results_100_repetition.csv'))
print(forced_choice_young_train_test_senior_validate_RBF_results)
sink()
```
```{r 29 young participants RBF LOO: tune, train, test, validation}
trainsetID <- vector(mode = "list", length = 10)
testsetID <- vector(mode = "list", length = 10)
result_names <- vector(mode = "list", length = 10)
result_decision <- vector(mode = "list", length = 10)
result_probability <- vector(mode = "list", length = 10)
acc_results <- vector(mode = "list", length = 10)
acc_table_results <- vector(mode = "list", length = 10)
acc_stats_results <- vector(mode = "list", length = 10)
acc_probA_results <- vector(mode = "list", length = 10)
acc_probB_results <- vector(mode = "list", length = 10)
acc_agreement_results <- vector(mode = "list", length = 10)
acc_agreement_table_results <- vector(mode = "list", length = 10)
acc_prop_table_results <- vector(mode = "list", length = 10)

validation_acc_results <- vector(mode = "list", length = 10)
validation_acc_table_results <- vector(mode = "list", length = 10)
validation_acc_stats_results <- vector(mode = "list", length = 10)
validation_acc_probA_results <- vector(mode = "list", length = 10)
validation_acc_probB_results <- vector(mode = "list", length = 10)
validation_acc_agreement_results <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results <- vector(mode = "list", length = 10)
validation_acc_prop_table_results <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions

  obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15), tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
  cost_val <- obj$best.parameters$cost
  gamma_val <- obj$best.parameters$gamma
# summary(obj)
# plot(obj)

  svm.model <- svm(sleep_deprivation~., data = train_set, cost = cost_val, gamma = gamma_val, tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
# grep("sleep_deprivation", colnames(train_set))

# test
  svm.pred <- predict(svm.model, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
  accuracy <- mean(svm.pred == test_set[,1])
  acc_table <- table(pred = svm.pred, true = test_set[,1])
  acc_stats <- classAgreement(table(pred = svm.pred, true = test_set[,1]))
#  acc_probA <- svm.model$probA
# 
#  acc_probB <- svm.model$probB
# 

  acc_agreement <- svm.pred == test_set[,1]
  acc_agreement_table <- table(acc_agreement)
  acc_prop_table <- prop.table(table(acc_agreement))
# FALSE  TRUE 
# store test results
  acc_results[[i]] <- accuracy
  acc_table_results[[i]] <- acc_table
  acc_stats_results[[i]] <- acc_stats
#  acc_probA_results[[i]] <- acc_probA
#  acc_probB_results[[i]] <- acc_probB
  acc_agreement_results[[i]] <- acc_agreement
  acc_agreement_table_results[[i]] <- acc_agreement_table
  acc_prop_table_results[[i]] <- acc_prop_table

# validation
  svm.pred_validation <- predict(svm.model, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy <- mean(svm.pred_validation == validation_set[,1])
  validation_acc_table <- table(pred = svm.pred_validation, true = validation_set[,1])
  validation_acc_stats <- classAgreement(table(pred = svm.pred_validation, true = validation_set[,1]))
#  validation_acc_probA <- svm.model$probA
#  
#  validation_acc_probB <- svm.model$probB
#
  validation_acc_agreement <- svm.pred_validation == validation_set[,1]
  validation_acc_agreement_table <- table(validation_acc_agreement)
  validation_acc_prop_table <- prop.table(table(validation_acc_agreement))
  
  validation_acc_results[[i]] <- validation_accuracy
  validation_acc_table_results[[i]] <- validation_acc_table
  validation_acc_stats_results[[i]] <- validation_acc_stats
#  validation_acc_probA_results[[i]] <- validation_acc_probA
#  validation_acc_probB_results[[i]] <- validation_acc_probB
  validation_acc_agreement_results[[i]] <- validation_acc_agreement
  validation_acc_agreement_table_results[[i]] <- validation_acc_agreement_table
  validation_acc_prop_table_results[[i]] <- validation_acc_prop_table
  
  # store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred)$names
  result_decision[[i]] <- attributes(svm.pred)$decision.values
  result_probability[[i]] <- attributes(svm.pred)$probabilities
}
young_train_test_senior_validate_RBF_LOO_results <- c(acc_results, acc_table_results, acc_stats_results, acc_agreement_results, acc_agreement_table_results, acc_prop_table_results, validation_acc_results, validation_acc_table_results, validation_acc_stats_results, validation_acc_agreement_results, validation_acc_agreement_table_results, validation_acc_prop_table_results)
sink(here("output",'230613_center_mean_young_train_test_senior_validate_RBF_LOO_10_repetition_results.csv'))
print(young_train_test_senior_validate_RBF_LOO_results)
sink()

forced_choice_young_train_test_senior_validate_RBF_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230612_forced_choice_young_train_test_senior_validate_RBF_LOO_10_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_RBF_results)
sink()
```

##### 29 centering linear models
```{r 29 young participants linear: tune, train, test, validation}
# cross-validation loop:
# split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
trainsetID <- vector(mode = "list", length = 100)
testsetID <- vector(mode = "list", length = 100)
result_names <- vector(mode = "list", length = 100)
result_decision <- vector(mode = "list", length = 100)
result_probability <- vector(mode = "list", length = 100)
acc_results_linear <- vector(mode = "list", length = 100)
acc_table_results_linear <- vector(mode = "list", length = 100)
acc_stats_results_linear <- vector(mode = "list", length = 100)
#acc_probA_results_linear <- vector(mode = "list", length = 100)
#acc_probB_results_linear <- vector(mode = "list", length = 100)
acc_agreement_results_linear <- vector(mode = "list", length = 100)
acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
acc_prop_table_results_linear <- vector(mode = "list", length = 100)

validation_acc_results_linear <- vector(mode = "list", length = 100)
validation_acc_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_stats_results_linear <- vector(mode = "list", length = 100)
#validation_acc_probA_results_linear <- vector(mode = "list", length = 100)
#validation_acc_probB_results_linear <- vector(mode = "list", length = 100)
validation_acc_agreement_results_linear <- vector(mode = "list", length = 100)
validation_acc_agreement_table_results_linear <- vector(mode = "list", length = 100)
validation_acc_prop_table_results_linear <- vector(mode = "list", length = 100)
  
for (i in 1:100) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

  obj_linear <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), kernel = "linear", probability = TRUE, decision.values = TRUE)
  cost_val <- obj_linear$best.parameters$cost
# train the model with the whole train_set and optimized parameters
  svm.model_linear <- svm(sleep_deprivation~., data = train_set, cost = cost_val, kernel = "linear", probability = TRUE, decision.values = TRUE)

# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_linear)
# test
  svm.pred_linear <- predict(svm.model_linear, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_linear <- mean(svm.pred_linear == test_set[,1], probability = TRUE, decision.values = TRUE)
  acc_table_linear <- table(pred = svm.pred_linear, true = test_set[,1])
  acc_stats_linear <- classAgreement(table(pred = svm.pred_linear, true = test_set[,1]))
#  acc_probA_linear <- svm.model_linear$probA
# 
#  acc_probB_linear <- svm.model_linear$probB
# 

  acc_agreement_linear <- svm.pred_linear == test_set[,1]
  acc_agreement_table_linear <- table(acc_agreement_linear)
  acc_prop_table_linear <- prop.table(table(acc_agreement_linear))
# FALSE  TRUE 
# store test results
  acc_results_linear[[i]] <- accuracy_linear
  acc_table_results_linear[[i]] <- acc_table_linear
  acc_stats_results_linear[[i]] <- acc_stats_linear
#  acc_probA_results_linear[[i]] <- acc_probA_linear
#  acc_probB_results_linear[[i]] <- acc_probB_linear
  acc_agreement_results_linear[[i]] <- acc_agreement_linear
  acc_agreement_table_results_linear[[i]] <- acc_agreement_table_linear
  acc_prop_table_results_linear[[i]] <- acc_prop_table_linear

# validation
  svm.pred_validation_linear <- predict(svm.model_linear, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy_linear <- mean(svm.pred_validation_linear == validation_set[,1])
  validation_acc_table_linear <- table(pred = svm.pred_validation_linear, true = validation_set[,1])
  validation_acc_stats_linear <- classAgreement(table(pred = svm.pred_validation_linear, true = validation_set[,1]))
#  validation_acc_probA_linear <- svm.model_linear$probA
#  
#  validation_acc_probB_linear <- svm.model_linear$probB
#
  validation_acc_agreement_linear <- svm.pred_validation_linear == validation_set[,1]
  validation_acc_agreement_table_linear <- table(validation_acc_agreement_linear)
  validation_acc_prop_table_linear <- prop.table(table(validation_acc_agreement_linear))
  
  validation_acc_results_linear[[i]] <- validation_accuracy_linear
  validation_acc_table_results_linear[[i]] <- validation_acc_table_linear
  validation_acc_stats_results_linear[[i]] <- validation_acc_stats_linear
#  validation_acc_probA_results_linear[[i]] <- validation_acc_probA_linear
#  validation_acc_probB_results_linear[[i]] <- validation_acc_probB_linear
  validation_acc_agreement_results_linear[[i]] <- validation_acc_agreement_linear
  validation_acc_agreement_table_results_linear[[i]] <- validation_acc_agreement_table_linear
  validation_acc_prop_table_results_linear[[i]] <- validation_acc_prop_table_linear
  
  # store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred_linear)$names
  result_decision[[i]] <- attributes(svm.pred_linear)$decision.values
  result_probability[[i]] <- attributes(svm.pred_linear)$probabilities
}
young_train_test_senior_validate_linear_results <- c(acc_results_linear, acc_table_results_linear, acc_stats_results_linear, acc_agreement_results_linear, acc_agreement_table_results_linear, acc_prop_table_results_linear, validation_acc_results_linear, validation_acc_table_results_linear, validation_acc_stats_results_linear, validation_acc_agreement_results_linear, validation_acc_agreement_table_results_linear, validation_acc_prop_table_results_linear)
sink(here("output",'230613_center_mean_young_train_test_senior_validate_linear_100_repetition_results.csv'))
print(young_train_test_senior_validate_linear_results)
sink()

forced_choice_young_train_test_senior_validate_linear_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230613_forced_choice_young_train_test_senior_validate_linear_100_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_linear_results)
sink()
```
```{r 29 young participants linear LOO: tune, train, test, validation}
trainsetID <- vector(mode = "list", length = 10)
testsetID <- vector(mode = "list", length = 10)
result_names <- vector(mode = "list", length = 10)
result_decision <- vector(mode = "list", length = 10)
result_probability <- vector(mode = "list", length = 10)
acc_results_linear <- vector(mode = "list", length = 10)
acc_table_results_linear <- vector(mode = "list", length = 10)
acc_stats_results_linear <- vector(mode = "list", length = 10)
acc_probA_results_linear <- vector(mode = "list", length = 10)
acc_probB_results_linear <- vector(mode = "list", length = 100)
acc_agreement_results_linear <- vector(mode = "list", length = 10)
acc_agreement_table_results_linear <- vector(mode = "list", length = 10)
acc_prop_table_results_linear <- vector(mode = "list", length = 10)

validation_acc_results_linear <- vector(mode = "list", length = 10)
validation_acc_table_results_linear <- vector(mode = "list", length = 10)
validation_acc_stats_results_linear <- vector(mode = "list", length = 10)
validation_acc_probA_results_linear <- vector(mode = "list", length = 10)
validation_acc_probB_results_linear <- vector(mode = "list", length = 10)
validation_acc_agreement_results_linear <- vector(mode = "list", length = 10)
validation_acc_agreement_table_results_linear <- vector(mode = "list", length = 10)
validation_acc_prop_table_results_linear <- vector(mode = "list", length = 10)
  
for (i in 1:10) {
  split <- splitTools::partition(TEST$ID,
                      p = c(train = 2/3, test = 1/3),
                      type = "grouped")
  train_set <- TEST[split$train,]
  test_set <- TEST[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# train_set & test_set have no overlapping subjects' datasets.
  train_set_ID <- train_set
  train_set <- train_set %>% 
    select(-ID)
  train_set <- as.data.frame(train_set)
  test_set_ID <- test_set
  test_set <- test_set %>% 
    select(-ID)
  test_set <- as.data.frame(test_set)

# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#  group_by(sleep_deprivation) %>% 
#  summarise(n())
# Sleep_deprivation: 0:12, 1:12
# pretty even split between the two conditions
# this line below appears to suggest all four kernels as best in different attempts
# https://stackoverflow.com/questions/61403058/how-to-tune-parameter-kernels-in-svm
# svmtune <- tune(svm, sleep_deprivation~., data=train_set, cost = 2^(-15:15), gamma = 2^(-15:15), coef0 = 2^(-15:15), ranges=list(kernel=c("sigmoid", "linear", "polynomial", "radial")))

  obj_linear <- tune.svm(sleep_deprivation~., data = train_set, type = "C-classification", cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
  cost_val <- obj_linear$best.parameters$cost
# train the model with the whole train_set and optimized parameters
  svm.model_linear <- svm(sleep_deprivation~., data = train_set, cost = cost_val, kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)), probability = TRUE, decision.values = TRUE)
# plotting
# View(train_set)
# plot(svm.model_linear, train_set, w_DMN_fc ~ w_DAN_fc)
# plot(svm.model_polynomial, train_set, w_DMN_fc ~ w_DAN_fc)

# grep("sleep_deprivation", colnames(train_set))
summary(svm.model_linear)
# test
  svm.pred_linear <- predict(svm.model_linear, test_set[,-1], probability = TRUE, decision.values = TRUE)

# evaluate test results
# need to find a way to run this result; caret::confusionMatrix(svm.pred_polynomial, test_set$sleep_deprivation)
  accuracy_linear <- mean(svm.pred_linear == test_set[,1])
  acc_table_linear <- table(pred = svm.pred_linear, true = test_set[,1])
  acc_stats_linear <- classAgreement(table(pred = svm.pred_linear, true = test_set[,1]))
#  acc_probA_linear <- svm.model_linear$probA
# 
#  acc_probB_linear <- svm.model_linear$probB
# 

  acc_agreement_linear <- svm.pred_linear == test_set[,1]
  acc_agreement_table_linear <- table(acc_agreement_linear)
  acc_prop_table_linear <- prop.table(table(acc_agreement_linear))
# FALSE  TRUE 
# store test results
  acc_results_linear[[i]] <- accuracy_linear
  acc_table_results_linear[[i]] <- acc_table_linear
  acc_stats_results_linear[[i]] <- acc_stats_linear
#  acc_probA_results_linear[[i]] <- acc_probA_linear
#  acc_probB_results_linear[[i]] <- acc_probB_linear
  acc_agreement_results_linear[[i]] <- acc_agreement_linear
  acc_agreement_table_results_linear[[i]] <- acc_agreement_table_linear
  acc_prop_table_results_linear[[i]] <- acc_prop_table_linear

# validation
  svm.pred_validation_linear <- predict(svm.model_linear, validation_set[,-1], probability = TRUE, decision.values = TRUE)
  validation_accuracy_linear <- mean(svm.pred_validation_linear == validation_set[,1])
  validation_acc_table_linear <- table(pred = svm.pred_validation_linear, true = validation_set[,1])
  validation_acc_stats_linear <- classAgreement(table(pred = svm.pred_validation_linear, true = validation_set[,1]))
#  validation_acc_probA_linear <- svm.model_linear$probA
#  
#  validation_acc_probB_linear <- svm.model_linear$probB
#
  validation_acc_agreement_linear <- svm.pred_validation_linear == validation_set[,1]
  validation_acc_agreement_table_linear <- table(validation_acc_agreement_linear)
  validation_acc_prop_table_linear <- prop.table(table(validation_acc_agreement_linear))
  
  validation_acc_results_linear[[i]] <- validation_accuracy_linear
  validation_acc_table_results_linear[[i]] <- validation_acc_table_linear
  validation_acc_stats_results_linear[[i]] <- validation_acc_stats_linear
#  validation_acc_probA_results_linear[[i]] <- validation_acc_probA_linear
#  validation_acc_probB_results_linear[[i]] <- validation_acc_probB_linear
  validation_acc_agreement_results_linear[[i]] <- validation_acc_agreement_linear
  validation_acc_agreement_table_results_linear[[i]] <- validation_acc_agreement_table_linear
  validation_acc_prop_table_results_linear[[i]] <- validation_acc_prop_table_linear
  
  # store extra for post-hoc forced choice
  trainsetID[[i]] <- train_set_ID$ID
  testsetID[[i]] <- test_set_ID$ID
  result_names[[i]] <- attributes(svm.pred_linear)$names
  result_decision[[i]] <- attributes(svm.pred_linear)$decision.values
  result_probability[[i]] <- attributes(svm.pred_linear)$probabilities  
}
young_train_test_senior_validate_linear_results <- c(acc_results_linear, acc_table_results_linear, acc_stats_results_linear, acc_agreement_results_linear, acc_agreement_table_results_linear, acc_prop_table_results_linear, validation_acc_results_linear, validation_acc_table_results_linear, validation_acc_stats_results_linear, validation_acc_agreement_results_linear, validation_acc_agreement_table_results_linear, validation_acc_prop_table_results_linear)
sink(here("output",'230613_center_mean_young_train_test_senior_validate_linear_LOO_10_repetition_results.csv'))
print(young_train_test_senior_validate_linear_results)
sink()

forced_choice_young_train_test_senior_validate_linear_results <- c(trainsetID, testsetID, result_names, result_decision, result_probability)
sink(here("output",'230613_forced_choice_young_train_test_senior_validate_linear_LOO_10_repetition_results.csv'))
print(forced_choice_young_train_test_senior_validate_linear_results)
sink()
```






# manuscript preparations
```{r citations}
citation()
# cite R
citation("e1071")
citation("splitTools")
# just copy and paste from the @Manual into the Import from clipboard option in Zotero
```

# ignore the below for now
```{r example testing}
# install.packages("mlbench")
data(Glass, package = "mlbench")
# ensure no double-dipping between testing and training data
index <- 1:nrow(Glass)
# need to figure the optimal way to separate the two dataset, now the default value is 3
# need to ensure that a same subject's visit 1 and 2 are always included in either the testing or training set, so they are not separated. Maybe refer to 4.4 simple splitting with important groups
# https://topepo.github.io/caret/data-splitting.html
# need to use subject-wise split instead of record-wise split (paper below)
# https://www-nature-com.libproxy.uoregon.edu/articles/s41746-019-0178-x
# https://community.rstudio.com/t/how-to-prevent-data-leakage-between-training-and-test-set-when-i-have-repeated-measures/30666/4
# https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
# https://stackoverflow.com/questions/52767833/train-test-split-with-repeated-measures
N <- trunc(length(index)/3)
testindex <- sample(index, N)
testset <- Glass[testindex,]
trainset <- Glass[-testindex,]
str(testset$Type)
"We don't have a separate function to do this for the train/test split. You could use sample() on the unique values of the group and allocate the rows with those group values to the test set.
"
# grid-search
# need to figure out how to do full grid search, here the combinations are very limited
# maybe don't worry too much about conducting a full grid search as too good parameters here may lead to over-fitting. I think we just need good enough. 
# I found this helpful info: "In practice, a logarithmic grid from to .01 to 1000 is usually sufficient. If the best parameters lie on the boundaries of the grid, it can be extended in that direction in a subsequent search." https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html
# I extend it to 15 to be consistent with Hsu et al.'s paper suggestion
# I used RBF kernel by default, also the most recommended one
####### to do: simple scaling should have been done with the e package, double check 
obj_test <- tune.svm(Type~., data = trainset, cost = 2^(-15:15), gamma = 2^(-15:15), 
                tunecontrol = tune.control(cross = nrow(trainset)))

####### to do: based on the paper, may need to follow up with a finer grid search with .25 increments, e.g., 2^3.25. First need to figure out the plot to understand how to select a finer grid range; also the Bayesian method mentioned in the previous link looks cool.

# https://rdrr.io/rforge/e1071/src/R/tune.R
# https://stat.ethz.ch/pipermail/r-help/2005-August/077427.html
# https://stats.stackexchange.com/questions/136274/leave-one-subject-out-cv-method

# sampling method: default is 10-fold cross validation, here I use leave one out with tune.control and this approach is more computationally costly, yet with more data to train
# article discussing the number of fold to select https://cran.r-project.org/web/packages/cvms/vignettes/picking_the_number_of_folds_for_cross-validation.html
# "(on average) the prediction error should be lower with a larger k... A higher number of folds means training a lot more models, which can be computationally heavy and time-consuming. So finding a lower k that yields a similar prediction error most of the time, can be very useful. "

summary(obj_test)
plot(obj_test)
# (The dependent variable, Type, has column number 10. cost is a general penalizing parameter for C-classification and gamma is the radial basis functionspecific kernel parameter.)
# use the best parameter to train the whole training set
svm.model_test <- svm(Type ~ ., data = trainset, cost = 8, gamma = .5)
# test
svm.pred_test <- predict(svm.model_test, testset[,-10])
table(pred = svm.pred_test, true = testset[,10])
classAgreement(table(pred = svm.pred_test, true = testset[,10]))
# https://stackoverflow.com/questions/62211003/r-studio-svm-classagreement-how-to
# https://rdrr.io/rforge/e1071/man/classAgreement.html

# data visualization
# https://www.statology.org/plot-svm-in-r/
plot(svm.model_test, trainset)

data(iris)
attach(iris)
x <- subset(iris, select = -Species) 
y <- Species 
model <- svm(x, y)
print(model)
summary(model)
pred <- predict(model, x)
table(pred, y)
pred <- predict(model, x, decision.values = TRUE)
attr(pred, "decision.values")[1:4,]
plot(cmdscale(dist(iris[,-5])), col = as.integer(iris[,5]), pch = c("o","+")[1:150 %in% model$index + 1])

obj <- tune(svm, Species~., data = iris, ranges = list(gamma = 2^(-1:1), cost = 2^(2:4)), tunecontrol = tune.control(sampling = "fix") )
```

## sleep-deprivation vulnerable subset, resistant subset validation
```{r future analyses subjective sleepiness participants sub-group analyses}
#names(participant)
# for preliminary analyses, evaluate how many participants are selected
participant_sl_manipulation <- participant %>% 
  select(conn_id, sl_cond, fell_asleep_session1, fell_asleep_session2) %>% 
  rename(ID = conn_id)
View(participant_sl_manipulation)

# filter(participant_sl_manipulation, grepl("Sure", sleep_deprived_session1))
  
# these variables (fell_asleep_session1, fell_asleep_session2) not used to select participants (I think they are too subjective) as Melynda suggested that we use Karolinska ratings, I would also use PVT to be more selective in later analyses. BTW, fell_asleep_session1, fell_asleep_session2 variables are used for blinding the experimenters, so shouldn't be used to select participants
# participant_sl_manipulation <- participant %>% 
#   select(conn_id, sl_cond, sleep_deprived_session1, sleep_deprived_session2) %>% 
#   rename(ID = conn_id)
# Karolinska Sleepiness Scale during 2nd resting scan
# 
# PVT
# # filter participants based on subjective sleepiness, dozing off, later PVT performance
# participant_label <- participant %>% 
#   filter(age_group == "Old") %>% 
#   select(conn_id) %>% 
#   rename(ID = conn_id)
# fc_s1_old <- inner_join(fc_s1, participant_label, by = "ID")
# fc_s2_old <- inner_join(fc_s2, participant_label, by = "ID")
# 
# #View(participant)
# # clarify sleep manipulation condition
# # originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# # new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# # str(fc_s1$sleep_deprivation)
# fc_s1_old_ml <- fc_s1_old %>% 
#   filter(index %in% selected_index) %>% 
#   pivot_wider(names_from = index, values_from = fc)
# fc_s2_old_ml <- fc_s2_old %>% 
#   filter(index %in% selected_index) %>% 
#   pivot_wider(names_from = index, values_from = fc)
# # visit/session 1, n=83; visit/session 2, n=79
# fc_old_ml <- bind_rows(fc_s1_old_ml, fc_s2_old_ml)
# test <- fc_old_ml %>% 
#   filter(!ID %in% c(22, 53, 66))
# 
# # split datasets, answer by Michael M: https://stats.stackexchange.com/questions/519391/split-data-into-test-training-and-validation-when-some-patients-have-multiple-o
# split <- splitTools::partition(test$ID,
#                       p = c(train = 2/3, test = 1/3),
#                       type = "grouped")
# train_set <- test[split$train,]
# test_set <- test[split$test,]
# describe(train_set)
# check1 <- unique(train_set$ID)
# check2 <- unique(test_set$ID)
# # https://datascience.stackexchange.com/questions/9317/how-to-get-common-values-between-two-multi-sets
# sum(check1 %in% check2)
# sum(check2 %in% check1)
# # train_set & test_set have no overlapping subjects' datasets.
# train_set <- train_set %>% 
#   select(-ID)
# train_set <- as.data.frame(train_set)
# 
# test_set <- test_set %>% 
#   select(-ID)
# test_set <- as.data.frame(test_set)
# 
# train_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# # Sleep_deprivation: 1(sleep deprived):23, 2(not sleep deprived):23
# test_set %>% 
#   group_by(sleep_deprivation) %>% 
#   summarise(n())
# # Sleep_deprivation: 0:12, 1:12
# # pretty even split between the two conditions
# 
# obj <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear")
# # .58, the above is the best so far, better than cost = 2^(2:4)
# obj_test <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), kernel = "linear", tunecontrol = tune.control(cross = nrow(train_set)))
# # .625 
# # check more complex models, not good performance
# obj_test <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(2:4), gamma = 2^(-1:1))
# obj_test <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(-15:15), gamma = 2^(-15:15))
# # the above is also .625 
# obj_test <- tune.svm(sleep_deprivation~., data = train_set, cost = 2^(2:4), gamma = 2^(-1:1), tunecontrol = tune.control(cross = nrow(train_set)))
# ####### to do: based on the paper, may need to follow up with a finer grid search with .25 increments, e.g., 2^3.25. First need to figure out the plot to understand how to select a finer grid range; also the Bayesian method mentioned in the previous link looks cool.
# 
# # https://rdrr.io/rforge/e1071/src/R/tune.R
# # https://stat.ethz.ch/pipermail/r-help/2005-August/077427.html
# # https://stats.stackexchange.com/questions/136274/leave-one-subject-out-cv-method
# 
# # sampling method: default is 10-fold cross validation, here I use leave one out with tune.control and this approach is more computationally costly, yet with more data to train
# # article discussing the number of fold to select https://cran.r-project.org/web/packages/cvms/vignettes/picking_the_number_of_folds_for_cross-validation.html
# # "(on average) the prediction error should be lower with a larger k... A higher number of folds means training a lot more models, which can be computationally heavy and time-consuming. So finding a lower k that yields a similar prediction error most of the time, can be very useful. "
# 
# summary(obj)
# plot(obj)
# 
# summary(obj_test)
# 
# # (The dependent variable, Type, has column number 10. cost is a general penalizing parameter for C-classification and gamma is the radial basis function specific kernel parameter.)
# # use the best parameter to train the whole training set
# # the more complex models above label all the cases as 2, hence ineffective
# svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = 0.03125, kernel = "linear", probability = TRUE)
# # .58
# # grep("sleep_deprivation", colnames(train_set))
# svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = 0.0625, kernel = "linear")
# # .625 
# 
# svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = 128, gamma = 0.0001220703)
# # .625
# svm.model <- svm(sleep_deprivation ~ ., data = train_set, cost = 4, gamma = .5, probability = TRUE)
# # always assign 1
# # test
# svm.pred <- predict(svm.model, test_set[,-1])
# table(pred = svm.pred, true = test_set[,1])
# classAgreement(table(pred = svm.pred, true = test_set[,1]))
# svm.model$probA
# # 
# svm.model$probB
# # 
# 
# agreement <- svm.pred == test_set[,1]
# table(agreement)
# prop.table(table(agreement))
# # FALSE  TRUE 
# #
# # validation
# participant_label <- participant %>% 
#   filter(age_group == "Young") %>% 
#   select(conn_id) %>% 
#   rename(ID = conn_id)
# fc_s1_young <- inner_join(fc_s1, participant_label, by = "ID")
# fc_s2_young <- inner_join(fc_s2, participant_label, by = "ID")
# 
# #View(participant)
# # clarify sleep manipulation condition
# # originally, sl_cond 1 and 2 refereed to during which session/visit the sleep deprivation occurred
# # new sleep_deprivation variable: 1: sleep deprived 2: rested wakefulness
# # str(fc_s1$sleep_deprivation)
# fc_s1_young_ml <- fc_s1_young %>% 
#   filter(index %in% selected_index) %>% 
#   pivot_wider(names_from = index, values_from = fc)
# fc_s2_young_ml <- fc_s2_young %>% 
#   filter(index %in% selected_index) %>% 
#   pivot_wider(names_from = index, values_from = fc)
# # visit/session 1, n=83; visit/session 2, n=79
# fc_young_ml <- bind_rows(fc_s1_young_ml, fc_s2_young_ml)
# test <- fc_young_ml %>% 
#   filter(!ID %in% c(22, 53, 66)) %>% 
#   select(-ID)
# test <- as.data.frame(test)
# 
# svm.pred <- predict(svm.model, test[,-1])
# table(pred = svm.pred, true = test[,1])
# classAgreement(table(pred = svm.pred, true = test[,1]))
# # 0.4651163; poor validation result (maybe due to age difference?)
# svm.model$probA
# # 
# svm.model$probB
# # 
# 
# agreement <- svm.pred == test[,1]
# table(agreement)
# prop.table(table(agreement))
```
### linear model, 10 fold per k
### linear model, LOO
### RBF model, 10 fold per k
### RBF model, LOO

